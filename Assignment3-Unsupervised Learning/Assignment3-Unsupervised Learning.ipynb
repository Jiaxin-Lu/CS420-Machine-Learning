{"cells":[{"metadata":{"id":"A16634FF19C74B7C876011AB5206D816","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"## 无监督学习 (Unsupervised Learning)\n\n相关算法：\n1. K-means聚类\n2. PCA主成分分析\n\n作业类型：\n1. 代码补全\n2. 简答题\n\n数据集：\nFashion-MNIST"},{"metadata":{"id":"529DE5089EE74DCC85A7010AF0E407D0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"### Fashion-MNIST数据集(Fashion-MNIST Dataset)\n\n来源：[A MNIST-like fashion product database](https://github.com/zalandoresearch/fashion-mnist)\n\nFashion-MNIST数据集由[Zalando](https://jobs.zalando.com/tech/)提出，与经典的[MNIST](http://yann.lecun.com/exdb/mnist/)数据集格式完全相同：包含10个种类，总计60000张训练图片和10000张测试图片，每张图片为28\\*28尺寸的灰度图片。\n\n我们借助tensorflow的接口读取数据集文件，并实现DataLoader类用于整理数据。"},{"metadata":{"id":"0BC4C599E9074ED28E651AF02ACF9C45","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[],"source":"from tensorflow.examples.tutorials.mnist import input_data\nimport random\nimport numpy as np\n\nclass DataLoader:\n    def __init__(self, folder):\n        self.dataset = input_data.read_data_sets(\"../input/\" + folder)\n        self.train_dataset = {}\n        self.test_dataset = {}\n        for images, labels, dataset in [(self.dataset.train.images, self.dataset.train.labels, self.train_dataset), \n                                        (self.dataset.validation.images, self.dataset.validation.labels, self.train_dataset), \n                                        (self.dataset.test.images, self.dataset.test.labels, self.test_dataset)]:\n            assert images.shape[0] == labels.shape[0]\n            for i in range(images.shape[0]):\n                if not labels[i] in dataset:\n                    dataset[labels[i]] = []\n                dataset[labels[i]].append(images[i])\n        \n    def load(self, label_list, train_n=-1, test_n=-1, shuffle=True):\n        train_list = []\n        test_list = []\n        new_label = 0\n        for label in label_list:\n            for image in self.train_dataset[label]:\n                train_list.append((image, new_label))\n            for image in self.test_dataset[label]:\n                test_list.append((image, new_label))\n            new_label += 1\n        if shuffle:\n            random.shuffle(train_list)\n            random.shuffle(test_list)\n        if train_n > 0:\n            train_list = train_list[: train_n]\n        if test_n > 0:\n            test_list = train_list[: test_n]\n        \n        return np.array([pair[0] for pair in train_list]), np.array([pair[1] for pair in train_list]), \\\n            np.array([pair[0] for pair in test_list]), np.array([pair[1] for pair in test_list]),\n                ","execution_count":88},{"metadata":{"id":"364138BD5D5F43538A09D5254F4D1ADD","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"实例化DataLoader并读取数据集文件。"},{"metadata":{"id":"8D06872F22424817AC289FFF412295C0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"Extracting ../input/Fashion_MNIST8667/train-images-idx3-ubyte.gz\nExtracting ../input/Fashion_MNIST8667/train-labels-idx1-ubyte.gz\nExtracting ../input/Fashion_MNIST8667/t10k-images-idx3-ubyte.gz\nExtracting ../input/Fashion_MNIST8667/t10k-labels-idx1-ubyte.gz\n","name":"stdout"}],"source":"dataLoader = DataLoader(\"Fashion_MNIST8667\")","execution_count":89},{"metadata":{"id":"A9DB942B24CA4B2DBA1629F655CEF11C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"读取所有数据，检查数据格式，所有图片均已向量化处理。"},{"metadata":{"id":"42AD1C401FF94273B3E3E644BD444C66","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"(60000, 784) (60000,)\n(10000, 784) (10000,)\n","name":"stdout"}],"source":"X_train, y_train, X_test, y_test = dataLoader.load([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":90},{"metadata":{"id":"91169693DBEE4B5A89275C722F2CF232","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"预览每种类别的部分图片。"},{"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 70 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/F2F9B1E01365453881B22610F8A65815/qaj5siz0af.png\">"},"transient":{}}],"execution_count":91,"source":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nclasses = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Boot']\nnum_classes = len(classes)\nsamples_per_class = 7\nfor y, cls in enumerate(classes):\n    idxs = np.flatnonzero(y_train == y)\n    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n    for i, idx in enumerate(idxs):\n        plt_idx = i * num_classes + y + 1\n        plt.subplot(samples_per_class, num_classes, plt_idx)\n        plt.imshow(X_train[idx].reshape((28,28)),cmap=plt.cm.gray)\n        plt.axis('off')\n        if i == 0:\n            plt.title(cls)\nplt.show()","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F2F9B1E01365453881B22610F8A65815","scrolled":false}},{"metadata":{"id":"16DC1242BC6D41B7AAFA45309B760690","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"我们选取其中的5个类别的图片进行K-means聚类学习。"},{"metadata":{"id":"586EA3ECEC104C9EAC4DED95CFDECD41","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"(30000, 784) (30000,)\n(5000, 784) (5000,)\n","name":"stdout"}],"source":"selected_classes = [0, 1, 2, 3, 4]\nn = len(selected_classes)\n\nX_train, y_train, X_test, y_test = dataLoader.load(selected_classes)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","execution_count":92},{"metadata":{"id":"9C854A3288224E3FB295DCA39C5C6CB2","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"### K-means聚类算法(k-means Clustering)\n\n实现最基本的K-means聚类算法。\n\n算法流程：\n1. 提供所需簇(cluster)的数量k。\n2. 随机选取k个实例作为种子节点，即作为每个簇的质心(centroid)。\n3. 迭代以下步骤：\n\t* 将每实例分配给最近质心相关联的簇。\n\t*\t重新估计每个簇的质心。\n4. 当聚类收敛时停止，或者在经过固定次数的迭代之后。\n\n**TODO：你需要补全K_Means类中fit函数的代码实现**\n\n代码解释：\n1. K_Means类中n_clusters变量为算法流程步骤1中的k，centroids为算法流程步骤2中的质心。\n2. fit函数参数列表中的max_iter为算法流程步骤4中的最大迭代次数，epsilon为收敛的阈值。\n\n要求：\n1. 实现算法流程中的所有步骤，包括质心的随机选取，迭代的收敛控制。\n2. 对fit函数的返回值没有特别要求，只需要将质心迭代结果存于centroids中，用于predict函数调用。\n3. 对质心的距离函数没有特别要求，可以尝试各种距离函数。"},{"metadata":{"id":"101DCE9A73534CDB9E53519AD6F6DB14","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[],"source":"class K_Means:\n    def __init__(self, n_clusters=5):\n        self.n_clusters = n_clusters\n        self.centroids = None\n    \n    def findClosestCentroids(self, X, centroids):\n        K = self.n_clusters\n        idx = np.zeros((X.shape[0],1))\n        temp = np.zeros((K, 1))\n        \n        for i in range(X.shape[0]):\n            for j in range(K):\n                dist = X[i,:] - centroids[j,:]\n                length = np.sum(dist**2)\n                temp[j] = length\n            idx[i] = np.argmin(temp) + 1\n        return idx\n        \n    def initCentroids(self, X, K):\n        m, n = X.shape[0], X.shape[1]\n        centroids = np.zeros((K,n))\n        \n        for i in range(K):\n            centroids[i] = X[np.random.randint(0, m+1), :]\n        print(\"init done\")\n        return centroids\n        \n    def computeCentroids(self, X, idx, K):\n        m, n = X.shape[0], X.shape[1]\n        centroids = np.zeros((K,n))\n        count = np.zeros((K,1))\n        for i in range(m):\n            index = int((idx[i]-1)[0])\n            centroids[index, :] += X[i, :]\n            count[index] += 1\n        return centroids/count\n    \n    def shouldStop(self, old_centroids, centroids, epsilon, i, max_iter):\n        if i > max_iter:\n            return True\n        return False\n        # if np.abs(old_centroids - centroids).all() < epsilon:\n        #     return True\n        # else:\n        #     return False\n    \n    def fit(self, X, max_iter=300, epsilon=0.01):\n        K = self.n_clusters\n        centroids = self.initCentroids(X, K)\n        \n        i = 0\n        old_centroids = np.zeros(centroids.shape)\n        while not self.shouldStop(old_centroids, centroids, epsilon, i, max_iter):\n            idx = self.findClosestCentroids(X, centroids)\n            old_centroids = centroids\n            centroids = self.computeCentroids(X, idx, K)\n            if i % 100 == 0:\n                print(\"iter %d\" % i)\n            i += 1\n        self.centroids = centroids\n        print(centroids)\n\n    def predict(self, X):\n        return np.array([np.argmin(np.diag(np.dot(self.centroids - X[i], (self.centroids - X[i]).T)))\n                        for i in range(X.shape[0])])","execution_count":93},{"metadata":{"id":"51C70BDD33334E59A811E448C1421761","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"在测试数据集上进行测试，并输出K-means聚类算法聚类分布。"},{"metadata":{"id":"54C16178136E4289849ABD755A15A5B8","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"init done\niter 0\niter 100\niter 200\niter 300\n[[0.00000000e+00 5.73481234e-05 3.43046043e-04 ... 5.44014711e-02\n  8.45519857e-03 4.95279234e-04]\n [1.28892978e-05 1.36948789e-05 4.10846370e-05 ... 1.37834929e-03\n  6.52520697e-05 5.15571920e-05]\n [5.91310142e-07 2.12871653e-05 1.59653740e-04 ... 7.64918795e-03\n  1.53858898e-03 2.22332610e-04]\n [0.00000000e+00 0.00000000e+00 0.00000000e+00 ... 0.00000000e+00\n  0.00000000e+00 0.00000000e+00]\n [4.90579380e-06 4.16992469e-05 3.39112994e-04 ... 1.74155677e-04\n  3.80199020e-05 7.35869066e-06]]\n[[ 34  14 315  21 616]\n [  3  13  85 849  50]\n [372 277 330   4  17]\n [  3  18 242 449 288]\n [249 487 144  23  97]]\n[[0.034 0.014 0.315 0.021 0.616]\n [0.003 0.013 0.085 0.849 0.05 ]\n [0.372 0.277 0.33  0.004 0.017]\n [0.003 0.018 0.242 0.449 0.288]\n [0.249 0.487 0.144 0.023 0.097]]\n","name":"stdout"}],"source":"k_means = K_Means(n_clusters=n)\nk_means.fit(X_train, max_iter=300, epsilon=0.001)\ny_predicted = k_means.predict(X_test)\nresult = np.zeros((n, n), dtype=int)\nfor i in range(X_test.shape[0]):\n    result[y_test[i]][y_predicted[i]] += 1\nprint(result)\nresult = result * 1.0\nfor i in range(n):\n    result[i] /= np.sum(result[i])\nprint(result)","execution_count":94},{"metadata":{"id":"851B61CF700B42E99EED05A89D806A3C","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"通过可视化直观表现聚类的分布情况。"},{"metadata":{"id":"9CA003ADD549448E83705206C3BD750B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 2 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9CA003ADD549448E83705206C3BD750B/qaj659fwa9.png\">"},"transient":{}}],"source":"plt.imshow(result)\nplt.colorbar()\nplt.show()","execution_count":95},{"metadata":{"id":"007B6D6A97FF417282953E0044F70A0B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"由于K-means聚类算法属于无监督学习算法，我们无法知晓每个质心和真实分类的对应关系，没有直观的正确率概念。这里我们假设最终每个质心分别与真实分类之间是一一对应关系，并通过枚举其对应关系得到一个自定义的正确率。\n\n**最终你实现的K-means聚类算法应该达到50%的正确率。**\n\n最终的学习结果可能存在一定的波动性。\n\n**问题**：对于如何改进K-means聚类算法，例如迭代速度、稳定性、避免局部最优等方面，你有什么想法？\n**答案**：比如在选取最开始的K个中心时，原始的方法是完全随机，这种方法可以更改成，假设已经选取了$n$个初始聚类中心$(0<n<K)$，则在选取第$n+1$个聚类中心时倾向于选取离当前$n$个聚类中心更远的点。这样可以更好地提高算法的稳定性，并且可以更快地收敛到好的聚类结果。\n现有的K-means聚类算法要求K要预先设定并且在训练的时候无法更改，可以考虑在训练的时候对聚类中心增加合并分裂的操作使得K值也可以根据训练结果进行调整。\n对于一些噪音维度可以考虑对K个聚类中心进行权重赋值，减少噪音对结果的影响。"},{"metadata":{"id":"4E682BDC188540948941216537619764","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"0.5131999999999999\n","name":"stdout"}],"source":"from itertools import permutations as perm\n\nscore = 0\nfor p in list(perm([i for i in range(n)])):\n    s = 0\n    for k in range(n):\n        s += result[k][p[k]]\n    score = max(score, s)\nprint(score / np.sum(result))","execution_count":96},{"metadata":{"id":"112FAC422F6947799EAD4213FE758239","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"### PCA主成分分析(Principal Components Analysis)\n\n基于特征值分解协方差矩阵方法实现PCA算法。\n\n算法流程：\n1. 确定原矩阵$\\mathbf{X}_{n \\times m}$以及主成分数量k。\n2. 对$\\mathbf{X}$的每一维去中心化，即减掉各自维度的平均值。\n3. 计算协方差矩阵$\\frac{1}{n}\\mathbf{X}^T\\mathbf{X}$的特征值和特征向量。\n4. 选取k个最大的特征值对应的特征向量，组成降维投影矩阵。\n5. 对原矩阵进行降维处理并输出，维度为$n \\times k$。\n\n**TODO：你需要补全pca函数的代码实现**\n\n代码解释：\n1. pca函数参数列表中的X和k为PCA主成分分析算法流程的步骤1中的原矩阵$\\mathbf{X}_{n \\times m}$以及主成分数量k。\n\n\n要求：\n1. 可以使用numpy库中的特征值和特征向量计算函数，但不允许直接调用sklearn库中的PCA相关函数。\n2. 可以不用统一每个维度的方差。"},{"metadata":{"id":"8EA0D60DFA9C4EBD8F28635442C55234","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[],"source":"def computeMean(X):\n    meanVal = np.mean(X, axis=0, keepdims=True)\n    x_mat = X - meanVal\n    return x_mat\n\ndef pca(X, k):\n    xMat = computeMean(X)\n    # covMat = np.dot(xMat.T, xMat) / xMat.shape[0]\n    covMat = np.cov(xMat, rowvar=0)\n    \n    eigVal, eigVect = np.linalg.eig(np.mat(covMat))\n    eigValInd = np.argsort(eigVal)\n    k_eigValInd = eigValInd[:-(k+1):-1]\n    k_eigVect = eigVect[:,k_eigValInd]\n    lowDimMat = np.dot(xMat, k_eigVect)\n    return np.array(lowDimMat)","execution_count":132},{"metadata":{"id":"460BC024A88B4193B40F28AE1F86BE7B","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"对PCA主成分分析算法的结果进行可视化处理。"},{"metadata":{"id":"4A78E713093B4F5BA9CDA8F8E1FB1D6F","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/4A78E713093B4F5BA9CDA8F8E1FB1D6F/qaj85emlfw.png\">"},"transient":{}}],"source":"X_input = np.concatenate([X_test, k_means.centroids])\nX_pca = pca(X_input, 2)\ncolor = ['r', 'b', 'g', 'y', 'c']\nfor i in range(n):\n    x_list = []\n    y_list = []\n    for j in range(X_test.shape[0]):\n        if y_predicted[j] == i:\n            x_list.append(X_pca[j][0])\n            y_list.append(X_pca[j][1])\n    plt.plot(x_list, y_list, '.', color=color[i])\n    plt.plot([X_pca[- n + i][0]], [X_pca[- n + i][1]], 's', color='k')\nplt.show()","execution_count":133},{"metadata":{"id":"9EFCB5FAB1A04BCA8A19E3D3AA43FCF9","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false,"trusted":true},"cell_type":"markdown","source":"与sklearn库中的PCA实现对比。\n\n**最终你实现的PCA主成分分析算法的结果应该与sklearn库的实现相似。**\n\n可能会存在旋转、缩放、镜像等差异，但拓扑关系应该保持一致。\n\n**问题**：对于PCA主成分分析算法进行数据处理的优缺点，你有什么想法？\n**答案**：PCA的优点主要是可以使要分析的数据的维度降低同时保留数据的主要信息，使得其结果更容易理解，方便去除噪声，并且方便识别最重要的多个特征，同时降低了后续算法的计算开销，以及PCA的过程没有参数的限制。\nPCA的缺点主要是可能损失一些可用的信息，并且对于有一定特征的数据无法通过参数化等方法对数据处理的过程进行干预，无法适用已有的先验知识。同时特征值的分解有一些局限性，对于变换的矩阵有要求。而在非高斯分布的情况下，PCA得到的主元不一定最优。"},{"metadata":{"id":"56207BB1F54A4D4685E3E59B3A9BF4EA","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false,"trusted":true},"cell_type":"code","outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/56207BB1F54A4D4685E3E59B3A9BF4EA/qaj97cpwca.png\">"},"transient":{}}],"source":"from sklearn.decomposition import PCA\n\nX_input = np.concatenate([X_test, k_means.centroids])\nX_pca = PCA(2).fit_transform(X_input)\ncolor = ['r', 'b', 'g', 'y', 'c']\nfor i in range(n):\n    x_list = []\n    y_list = []\n    for j in range(X_test.shape[0]):\n        if y_predicted[j] == i:\n            x_list.append(X_pca[j][0])\n            y_list.append(X_pca[j][1])\n    plt.plot(x_list, y_list, '.', color=color[i])\n    plt.plot([X_pca[- n + i][0]], [X_pca[- n + i][1]], 's', color='k')\nplt.show()","execution_count":134},{"metadata":{"id":"EF3EDF14D43D43678441A5457CA144C0","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"trusted":true},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}