{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"349DCCCB343F4B649A23AFB0E1822FAA","mdEditEnable":false},"source":"# Logistic Regression\n逻辑回归模型是一种常用的分类模型，它通过sigmoid或者softmax函数，将函数值映射到(0, 1)区间内，从而实现对样本的分类。在这个小作业中，你需要实现：\n1. 二分类和多分类两种逻辑回归模型\n2. 分别含有 L1 和 L2 两种正则项的损失函数，并计算对应的梯度\n3. 权重参数W的更新\n4. 比较不同的学习率对损失函数和分类器性能的影响\n5. 比较不同的正则项参数对于分类器性能的影响"},{"metadata":{"id":"2A7C0A404A2648F4B4D37BD564BF077E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip/http' or its parent directory is not owned by the current user and the cache has been disabled. Please check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n\u001b[33mWARNING: The directory '/home/jovyan/.cache/pip' or its parent directory is not owned by the current user and caching wheels has been disabled. check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\nCollecting sklearn\n  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn) (0.21.3)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.13.2)\nRequirement already satisfied: scipy>=0.17.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\nRequirement already satisfied: numpy>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.17.2)\nBuilding wheels for collected packages: sklearn\n\u001b[33m  WARNING: Building wheel for sklearn failed: [Errno 13] Permission denied: '/home/jovyan/.cache/pip/wheels'\u001b[0m\nFailed to build sklearn\nInstalling collected packages: sklearn\n  Running setup.py install for sklearn ... \u001b[?25ldone\n\u001b[?25hSuccessfully installed sklearn-0.0\n","name":"stdout"}],"source":"!pip install sklearn","execution_count":1},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"81849DCA00F1484AA4EFCB7C8BA3CBDC","mdEditEnable":false},"source":"## 一、二分类逻辑回归：\n### 1.1数据集介绍\n这个任务中使用的数据集是手写数字集MNIST，它有50000个训练样本和10000个测试样本，共10个类别。在二分类任务上，我们对MNIST数据集进行了一个采样，抽取了数据集中的‘5’和‘3’对应的样本作为二分类的正负样本，共得到10842个训练样本，1784个测试样本，其中正负样本数量均相同。为了让大家对于这个数据集有一个更直观的认识，我们从正负样本中各抽取了8个样例进行了可视化。"},{"cell_type":"code","execution_count":2,"metadata":{"pycharm":{"is_executing":false},"scrolled":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"7ED97409753E411AAE7237C565E7E95A","collapsed":false},"outputs":[],"source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n\ndef load_data1(path):\n    # load all MNIST data\n    fd = open(os.path.join(path, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    train_X_all = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n    fd = open(os.path.join(path, 'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    train_Y_all = loaded[8:].reshape(60000).astype(np.float)\n    fd = open(os.path.join(path, 't10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    test_X_all = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n    fd = open(os.path.join(path, 't10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    test_Y_all = loaded[8:].reshape(10000).astype(np.float)\n\n    #subsample data\n    train_idxs_df = pd.read_csv(os.path.join(path, 'train_indices.csv'))\n    test_idxs_df = pd.read_csv(os.path.join(path, 'test_indices.csv'))\n    pos_train_indices = train_idxs_df['pos_train_indices'].tolist()\n    neg_train_indices = train_idxs_df['neg_train_indices'].tolist()\n    pos_test_indices = test_idxs_df['pos_test_indices'].tolist()\n    neg_test_indices = test_idxs_df['neg_test_indices'].tolist()\n    train_Y_all[pos_train_indices] = 1\n    train_Y_all[neg_train_indices] = 0\n    test_Y_all[pos_test_indices] = 1\n    test_Y_all[neg_test_indices] = 0\n    train_indices = np.append(pos_train_indices, neg_train_indices)\n    test_indices = np.append(pos_test_indices, neg_test_indices)\n    train_X = train_X_all[train_indices]\n    train_Y = train_Y_all[train_indices]\n    test_X = test_X_all[test_indices]\n    test_Y = test_Y_all[test_indices]\n\n    #visualiza data\n    sample_num = 8\n    pos_sample_indices = np.random.choice(pos_train_indices, sample_num, replace=False)\n    neg_sample_indices = np.random.choice(neg_train_indices, sample_num, replace=False)\n    for i, idx in enumerate(pos_sample_indices):\n        plt_idx = i + 1\n        plt.subplot(2, sample_num, plt_idx)\n        plt.imshow(train_X_all[idx, :, :, :].reshape((28, 28)), cmap=plt.cm.gray)\n        plt.axis('off')\n        if i == 0:\n            plt.title('Positive')\n\n    for i, idx in enumerate(neg_sample_indices):\n        plt_idx = sample_num + i + 1\n        plt.subplot(2, sample_num, plt_idx)\n        plt.imshow(train_X_all[idx, :, :, :].reshape((28, 28)), cmap=plt.cm.gray)\n        plt.axis('off')\n        if i == 0:\n            plt.title('Negative')\n    \n    # reshaple into rows and normaliza\n    train_X = train_X.reshape((train_X.shape[0], -1))\n    test_X = test_X.reshape((test_X.shape[0], -1))\n    mean_image = np.mean(train_X, axis=0)\n    train_X = train_X - mean_image\n    test_X = test_X - mean_image\n\n    # add a bias columu into X\n    train_X = np.hstack([train_X, np.ones((train_X.shape[0], 1))])\n    test_X = np.hstack([test_X, np.ones((test_X.shape[0], 1))])\n    return train_X, train_Y, test_X, test_Y\n\n\nX_train, Y_train, X_test, Y_test = load_data1('/home/kesci/input/MNIST_dataset4284')"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0ADF94CCCE824695A9814D1730001A8D","mdEditEnable":false},"source":"### 1.2逻辑回归模型\n在这一部分中你需要完成以下内容：\n1. train函数中权重的更新\n2. L1和L2两种正则化的损失函数及对应梯度的计算\n3. predict函数中的预测类别的计算"},{"cell_type":"code","execution_count":7,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C0AAD39A300446338D60432B301DF402","collapsed":false,"scrolled":false},"outputs":[],"source":"def sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nclass LinearRegression1(object):\n    def __init__(self):\n        self.W = None\n    \n    def train(self, X, Y, learning_rate=1e-3, reg=1e-5, reg_type='L2', num_iters=2000,\n          batch_size=128, display = True, new_model = True):\n        num_train, feat_dim = X.shape\n        if new_model :\n            self.W = 0.001 * np.random.randn(feat_dim)\n        loss_history = []\n        for i in range(num_iters):\n            batch_indices = np.random.choice(num_train, batch_size, replace=True)\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            if reg_type == 'L1':\n                loss, grad = self.l1_loss(X_batch, Y_batch, reg)\n            else:\n                loss, grad = self.l2_loss(X_batch, Y_batch, reg)\n            loss_history.append(loss)\n            \n            #TODO:update W\n            self.W -= learning_rate * grad\n\n            #End of your code\n            \n            if display and i % 100 == 0:\n                print(\"In iteration {}/{} , the loss is {}\".format(i, num_iters, loss))\n        return loss_history\n\n    def calc_loss_grad(self, X, Y):\n        batch_size, dim = X.shape\n\n        z = sigmoid(np.dot(X, np.transpose(self.W)))\n        loss = -1 * (np.dot(np.squeeze(Y), np.log(z)) + np.dot((1 - np.squeeze(Y)), np.log(1 - z)))\n        w_grad = np.sum(-1 * X * (np.squeeze(Y) - z).reshape((batch_size,1)), axis=0)\n        return loss / batch_size , w_grad / batch_size\n\n    \n    def l1_loss(self, X, Y, reg):\n        # loss = 0\n        # grad = None\n        \n        #TODO: Calculate the loss and gradient\n\n        loss, grad = self.calc_loss_grad(X, Y)\n        loss = loss + reg * np.sum(abs(self.W))\n        grad = grad + reg\n\n        # End of your code\n        \n        return loss, grad\n    \n    def l2_loss(self, X, Y, reg):\n        loss = 0\n        grad = None\n        \n        #TODO: Calculate the loss and gradient\n        \n        loss, grad = self.calc_loss_grad(X, Y)\n        loss = loss + reg * np.sum(self.W * self.W)\n        grad = grad + 2 * reg * self.W\n        \n        # End of your code\n        \n        return loss, grad\n    \n    def predict(self, X, threshold=0.5):\n        Y_pred = np.zeros(X.shape[0])\n\n        #TODO: Calculate the prediction\n        # Y_pred = sigmoid(np.dot(X, self.W))\n        for i in range(X.shape[0]):\n            ans = sigmoid(np.sum(self.W * X[i]))\n            if (ans >= threshold):\n                Y_pred[i] = 1\n            else:\n                Y_pred[i] = 0;\n        \n        #End of your code\n        \n        return Y_pred\n        \n    def draw_W(self):\n        Xmin = np.min(self.W)\n        Xmax = np.max(self.W)\n        plt.hist(self.W,bins=50)\n        plt.xlabel('Area')\n        plt.xlim(Xmin, Xmax)\n        plt.ylabel('Number')\n        # plt.ylim(0,100)\n        plt.title('W')\n        plt.show()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C860E4A6D2084E68B890076E8105D4C5","mdEditEnable":false},"source":"### 1.3 训练模型实例\n在这一部分，你不需要完成任何代码，你可以通过这一部分验证你上面实现的LogisticRegression1的代码是否实现正确。"},{"cell_type":"code","execution_count":8,"metadata":{"scrolled":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8A4053C3BFC742D1A2A0326917E2B4C9","collapsed":false},"outputs":[{"output_type":"stream","text":"In iteration 0/2000 , the loss is 0.8994590522777538\nIn iteration 100/2000 , the loss is 0.17534203681480814\nIn iteration 200/2000 , the loss is 0.12262025762205832\nIn iteration 300/2000 , the loss is 0.11586265532277332\nIn iteration 400/2000 , the loss is 0.1751349401935428\nIn iteration 500/2000 , the loss is 0.1191054906240259\nIn iteration 600/2000 , the loss is 0.06237859137078771\nIn iteration 700/2000 , the loss is 0.07289062233622569\nIn iteration 800/2000 , the loss is 0.14250482917110485\nIn iteration 900/2000 , the loss is 0.12832735809462315\nIn iteration 1000/2000 , the loss is 0.12392580940045864\nIn iteration 1100/2000 , the loss is 0.18576922639877577\nIn iteration 1200/2000 , the loss is 0.16336939296353306\nIn iteration 1300/2000 , the loss is 0.1318461928838651\nIn iteration 1400/2000 , the loss is 0.11914072393778617\nIn iteration 1500/2000 , the loss is 0.11592232373477075\nIn iteration 1600/2000 , the loss is 0.13956918988974104\nIn iteration 1700/2000 , the loss is 0.1882600350357967\nIn iteration 1800/2000 , the loss is 0.09467047787597073\nIn iteration 1900/2000 , the loss is 0.06675529028588377\nThe Accuracy is 0.95347533632287\n\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8A4053C3BFC742D1A2A0326917E2B4C9/q7myiecund.png\">"},"transient":{}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/8A4053C3BFC742D1A2A0326917E2B4C9/q7myiem0bj.png\">"},"transient":{}}],"source":"lr_param = 1.5e-6\nreg_param = 0.01\n\nmodel = LinearRegression1()\nloss_history = model.train(X_train, Y_train, lr_param, reg_param, 'L2')\npred = model.predict(X_test)\nacc = np.mean(pred == Y_test)\nprint(\"The Accuracy is {}\\n\".format(acc))\n\nmodel.draw_W()\n\nx = range(len(loss_history))\nplt.plot(x, loss_history, label='Loss')\nplt.legend()\nplt.xlabel('Iteration Num')\nplt.ylabel('Loss')\nplt.show()\nW = model.W"},{"metadata":{"id":"27A95F6A3315482A8EF1F4DFE3A0025E","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 1.4 学习率和Loss函数、模型性能的关系\n因为学习率和正则化参数都是超参数，在一般的训练过程中，我们没办法直接优化，所以我们一般会将训练集细分成训练集和验证集，然后通过模型在验证集上的表现选择一个最优的超参数，再将它对应的最优的模型应用到测试集中。\n在这一部分你需要完成以下内容：\n1. 尝试多种不同的学习率\n2. 储存学习率对应的损失函数值到L1_loss和L2_loss中（我们对损失函数值进行了20步平均化处理）。\n3. 储存学习率对应的**在验证集上**的正确率到L1_lr_val_acc和L2_lr_val_acc中\n\n#### 注意：\n因为已有代码中L1_loss，L1_lr_val_acc都是数组，在可视化的过程中我们需要学习率和它们相对应，比如learning_rates[0]对应的loss和validation accuracy应该储存在数组index为0的位置\n\n#### 拓展：\n在这个部分中采取的损失函数都是定值，如果你有时间的话，可以尝试根据迭代轮数改变学习率，并比较不变的学习率和变化的学习率对于模型性能的影响。"},{"metadata":{"id":"29B73EBF7A1A49A7AB8EF67379778334","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:36: RuntimeWarning: divide by zero encountered in log\n","name":"stderr"},{"output_type":"stream","text":"L1 7e-05 0.9493670886075949\nL1 5e-05 0.9479606188466948\nL1 3e-05 0.9521800281293952\nL1 1e-05 0.9549929676511955\nL1 7e-06 0.9549929676511955\nL1 5e-06 0.9549929676511955\nL1 3e-06 0.9479606188466948\nL1 1e-06 0.939521800281294\nL1 7e-07 0.9409282700421941\nL2 7e-05 0.9479606188466948\nL2 5e-05 0.9507735583684951\nL2 3e-05 0.9493670886075949\nL2 1e-05 0.9507735583684951\nL2 7e-06 0.9507735583684951\nL2 5e-06 0.9535864978902954\nL2 3e-06 0.9521800281293952\nL2 1e-06 0.9409282700421941\nL2 7e-07 0.9381153305203939\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/29B73EBF7A1A49A7AB8EF67379778334/q7g31qj8en.png\">"},"transient":{}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/29B73EBF7A1A49A7AB8EF67379778334/q7g31qxe7x.png\">"},"transient":{}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/29B73EBF7A1A49A7AB8EF67379778334/q7g31qqtyo.png\">"},"transient":{}}],"source":"reg = 0.01\nreg_types = ['L1', 'L2']\nL1_loss = []\nL2_loss = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nL1_lr_val_acc = []\nL2_lr_val_acc = []\n\n#TODO:\nlearning_rates = [7e-5, 5e-5, 3e-5, 1e-5, 7e-6, 5e-6, 3e-6, 1e-6, 7e-7]\n\nfor reg_type in range(len(reg_types)):\n    for i in range(len(learning_rates)):\n        model = LinearRegression1()\n        learning_rate = learning_rates[i]\n        loss_history = model.train(X_train, Y_train, learning_rate, reg, reg_types[reg_type], display = False)\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n        \n        if reg_type == 0:\n            L1_loss.append(loss_history)\n            L1_lr_val_acc.append(accuracy)\n        else:\n            L2_loss.append(loss_history)\n            L2_lr_val_acc.append(accuracy)\n            \n        print(reg_types[reg_type], learning_rate, accuracy)\n#End of your code\n\n#visulize the relationship between lr and loss(L1)\nfor i, lr in enumerate(learning_rates):\n    L1_loss_label = str(lr) + 'L1'\n    # L2_loss_label = str(lr) + 'L2'\n    L1_loss_i = L1_loss[i]\n    # L2_loss_i = L2_loss[i]\n    ave_L1_loss = np.zeros_like(L1_loss_i)\n    # ave_L2_loss = np.zeros_like(L2_loss_i)\n    ave_step = 20\n    for j in range(len(L1_loss_i)):\n        if j < ave_step:\n            ave_L1_loss[j] = np.mean(L1_loss_i[0: j + 1])\n            # ave_L2_loss[j] = np.mean(L2_loss_i[0: j + 1])\n        else:\n            ave_L1_loss[j] = np.mean(L1_loss_i[j - ave_step + 1: j + 1])    \n            # ave_L2_loss[j] = np.mean(L2_loss_i[j - ave_step + 1: j + 1])\n    x = range(len(L1_loss_i))\n    plt.plot(x, ave_L1_loss, label=L1_loss_label)\n    # plt.plot(x, ave_L2_loss, label=L2_loss_label)\n    \nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Loss')\nplt.show()\n\n#visulize the relationship between lr and loss(L2)\nfor i, lr in enumerate(learning_rates):\n    # L1_loss_label = str(lr) + 'L1'\n    L2_loss_label = str(lr) + 'L2'\n    # L1_loss_i = L1_loss[i]\n    L2_loss_i = L2_loss[i]\n    # ave_L1_loss = np.zeros_like(L1_loss_i)\n    ave_L2_loss = np.zeros_like(L2_loss_i)\n    ave_step = 20\n    for j in range(len(L1_loss_i)):\n        if j < ave_step:\n            # ave_L1_loss[j] = np.mean(L1_loss_i[0: j + 1])\n            ave_L2_loss[j] = np.mean(L2_loss_i[0: j + 1])\n        else:\n            # ave_L1_loss[j] = np.mean(L1_loss_i[j - ave_step + 1: j + 1])    \n            ave_L2_loss[j] = np.mean(L2_loss_i[j - ave_step + 1: j + 1])\n    x = range(len(L2_loss_i))\n    # plt.plot(x, ave_L1_loss, label=L1_loss_label)\n    plt.plot(x, ave_L2_loss, label=L2_loss_label)\n    \nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Loss')\nplt.show()\n\n#visulize the relationship between lr and accuracy\nx = range(len(learning_rates))\nplt.plot(x, L1_lr_val_acc, label='L1_val_acc')\nplt.plot(x, L2_lr_val_acc, label='L2_val_acc')\nplt.xticks(x, learning_rates)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Validation Accuracy')\nplt.show()","execution_count":24},{"metadata":{"id":"DA6289D7D8FF43C1A3257E3FC684E856","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"#### Question1: 学习率和损失函数的变化、模型性能之间分别有什么关系？\nAnswer1:由图表中可以看到，对于不同的学习率，损失函数均是先极速下降后平缓下降，最后在一定范围内震荡变化。同时，从图表中可以看到，损失函数最后收敛的值基本符合随着学习率的减小而增大的规律（这一规律在学习率较大的情况下较不明显，在学习率较小的情况下较为明显）。（由于图像线条过于密集，故将L1和L2的结果分开表示）。\n而模型的性能随着学习率的变化有所变化。整体呈现上凸形，即在一定范围内模型性能更好，当学习率高于或低于这一范围，则模型性能会较为明显地下降。从图表中可以看到对于L1和L2两种回归模型，这一范围大概为$[5e-06,3e-05]$这一区间。"},{"metadata":{"id":"E0F04884CCCE45079E4249B1A1D820C6","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"#### 拓展\n以下代码尝试了根据迭代轮数改变学习率\n变化的学习率可以一定程度上做到比不变的学习率的准确性更高，而且这需要更为精确的对轮次和学习率的把握。"},{"metadata":{"id":"11634550555946D7855133FF04D4FC7D","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"accuracy_altr now is 0.9626373626373627\naccuracy_altr now is 0.967032967032967\naccuracy_altr now is 0.9692307692307692\naccuracy_altr now is 0.967032967032967\nTotal accuracy is 0.967032967032967\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/11634550555946D7855133FF04D4FC7D/q7g5gdqqj6.png\">"},"transient":{}}],"source":"L2_altr_loss = []\nlearning_rates_step = [1e-5, 8e-6, 6e-6, 5e-6]\nnum_iter = 500\nmodel = LinearRegression1()\nfor i in range(len(learning_rates_step)):\n    learning_rate = learning_rates_step[i]\n    if i == 0:\n        L2_altr_loss += model.train(X_train, Y_train, learning_rate, reg, 'L2',\n            num_iters = num_iter, display = False, new_model = True)\n    else:\n        L2_altr_loss += model.train(X_train, Y_train, learning_rate, reg, 'L2',\n            num_iters = num_iter, display = False, new_model = False)\n\n    accuracy_altr = np.mean(model.predict(X_val) == Y_val)\n    print(\"accuracy_altr now is {}\".format(accuracy_altr))\n    \n\naccuracy = np.mean(model.predict(X_val) == Y_val)\nprint(\"Total accuracy is {}\".format(accuracy))\n#visulize l2_altr_loss\nx = range(len(L2_altr_loss))\nplt.plot(x, L2_altr_loss, label='Loss')\nplt.legend()\nplt.xlabel('Iteration Num')\nplt.ylabel('Loss')\nplt.show()","execution_count":49},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5CBDD6A9235E466482D04B6E7D0C6D5F","mdEditEnable":false},"source":"### 1.5 正则项与模型性能\n在这一部分中，你需要完成以下内容：\n1. 尝试多个正则化参数的值\n2. 储存对应的在**验证集上**的正确率到L1_reg_val_acc和L2_reg_val_acc中\n3. 通过验证集X_val和Y_val选择最优的正则化超参数，并储存最优正则化参数和对应模型\n\n已有的代码会画出正则化参数和验证集上正确率的关系图，并计算最优的模型在测试集上的正确率。\n\n#### 注意：\n和上面学习率一样，L1_reg_val_acc的存储也需要和正则化参数值对应。"},{"cell_type":"code","execution_count":26,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"69F1B83300954853826A6383607380B2","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"L1 0.001 0.9494505494505494\nL1 0.005 0.9582417582417583\nL1 0.01 0.9560439560439561\nL1 0.015 0.9560439560439561\nL1 0.02 0.9560439560439561\nL1 0.025 0.9538461538461539\nL1 0.03 0.9626373626373627\nL1 0.035 0.9494505494505494\nL2 0.001 0.9494505494505494\nL2 0.005 0.9538461538461539\nL2 0.01 0.9560439560439561\nL2 0.015 0.9560439560439561\nL2 0.02 0.9538461538461539\nL2 0.025 0.9538461538461539\nL2 0.03 0.9516483516483516\nL2 0.035 0.9472527472527472\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/69F1B83300954853826A6383607380B2/q7g3pruwhc.png\">"},"transient":{}},{"output_type":"stream","text":"The Accuracy with L1 regularization parameter 0.03 is 0.9540358744394619\n\nThe Accuracy with L2 regularization parameter 0.01 is 0.9568385650224215\n\n","name":"stdout"}],"source":"learning_rate = 1.5e-6\nreg_types = ['L1', 'L2']\nL1_reg_val_acc = []\nL2_reg_val_acc = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nbest_L1_model = None\nbest_L2_model = None\nbest_L1_reg = 0\nbest_L2_reg = 0\n\n#TODO\nregs = [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035]\n\n#End of your code\nfor reg_type in range(len(reg_types)):\n    best_accuracy = 0\n    for i in range(len(regs)):\n        model = LinearRegression1()\n        reg = regs[i]\n        loss_history = model.train(X_train, Y_train, learning_rate, reg, reg_types[reg_type], display = False)\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n\n        if reg_type == 0:\n            L1_reg_val_acc.append(accuracy)\n            if (accuracy > best_accuracy):\n                best_accuracy = accuracy\n                best_L1_model = model\n                best_L1_reg = reg\n        else:\n            L2_reg_val_acc.append(accuracy)\n            if (accuracy > best_accuracy):\n                best_accuracy = accuracy\n                best_L2_model = model\n                best_L2_reg = reg\n        print(reg_types[reg_type], reg, accuracy)\n\n#visuliza the relation of regularization parameter and validation accuracy\nx = range(len(regs))\nplt.plot(x, L1_reg_val_acc, label='L1_val_acc')\nplt.plot(x, L2_reg_val_acc, label='L2_val_acc')\nplt.xticks(x, regs)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter reg')\nplt.ylabel('Validation Accuracy')\nplt.show()\n\n#Compute the performance of best model on the test set\nL1_pred = best_L1_model.predict(X_test)\nL1_acc = np.mean(L1_pred == Y_test)\nprint(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L1_reg, L1_acc))\nL2_pred = best_L2_model.predict(X_test)\nL2_acc = np.mean(L2_pred == Y_test)\nprint(\"The Accuracy with L2 regularization parameter {} is {}\\n\".format(best_L2_reg, L2_acc))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"FE5B019BDA934812B195721468F03A19","mdEditEnable":false},"source":"## 二、多分类逻辑回归"},{"metadata":{"id":"CDA14DC4417C433280B811EB721C78AB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 2.1 加载数据集"},{"cell_type":"code","execution_count":9,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"2435572FAB2F49589F97188F6F3E00F3","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 80 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/2435572FAB2F49589F97188F6F3E00F3/q7myn5gfz7.png\">"},"transient":{}}],"source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\n\ndef load_data2(path):\n    # load all MNIST data\n    fd = open(os.path.join(path, 'train-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    train_X = loaded[16:].reshape((60000, 28, 28, 1)).astype(np.float)\n    fd = open(os.path.join(path, 'train-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    train_Y = loaded[8:].reshape(60000).astype(np.float)\n    fd = open(os.path.join(path, 't10k-images-idx3-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    test_X = loaded[16:].reshape((10000, 28, 28, 1)).astype(np.float)\n    fd = open(os.path.join(path, 't10k-labels-idx1-ubyte'))\n    loaded = np.fromfile(file=fd, dtype=np.uint8)\n    test_Y = loaded[8:].reshape(10000).astype(np.float)\n\n    #visualiza data\n    sample_num = 8\n    num_classes = 10\n    for y in range(num_classes):\n        idxs = np.flatnonzero(train_Y == y)\n        idxs = np.random.choice(idxs, sample_num, replace=False)\n        for i, idx in enumerate(idxs):\n            plt_idx = i * num_classes + y + 1\n            plt.subplot(sample_num, num_classes, plt_idx)\n            plt.imshow(train_X[idx, :, :, :].reshape((28,28)),cmap=plt.cm.gray)\n            plt.axis('off')\n            if i == 0:\n                plt.title(y)\n    plt.show()\n\n    # reshaple into rows and normaliza\n    train_X = train_X.reshape((train_X.shape[0], -1))\n    test_X = test_X.reshape((test_X.shape[0], -1))\n    mean_image = np.mean(train_X, axis=0)\n    train_X = train_X - mean_image\n    test_X = test_X - mean_image\n\n    # add a bias columu into X\n    train_X = np.hstack([train_X, np.ones((train_X.shape[0], 1))])\n    test_X = np.hstack([test_X, np.ones((test_X.shape[0], 1))])\n    train_Y = train_Y.astype(np.int32)\n    test_Y = test_Y.astype(np.int32)\n    return train_X, train_Y, test_X, test_Y\n\n\nX_train, Y_train, X_test, Y_test = load_data2('/home/kesci/input/MNIST_dataset4284')"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"CDE7A7DC8F9542EC93B98D91D6C0AA01","mdEditEnable":false},"source":"### 2.2逻辑回归模型\n在这一部分中你需要完成与二分类逻辑回归相同的任务。"},{"cell_type":"code","execution_count":10,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6A55B828E6AF46D88C07A902A8D51A51","collapsed":false,"scrolled":false},"outputs":[],"source":"def softmax(x, axis=1):\n    row_max = x.max(axis=axis)\n    row_max=row_max.reshape(-1, 1)\n    x = x - row_max\n    x_exp = np.exp(x)\n    x_sum = np.sum(x_exp, axis=axis, keepdims=True)\n    return x_exp / x_sum\n    \nclass LinearRegression2(object):\n    def __init__(self):\n        self.W = None\n    def train(self, X, Y, learning_rate=1e-3, reg=1e-5, reg_type='L2', num_iters=2000,\n              batch_size=128, display = True, new_model = True):\n        num_train, feat_dim = X.shape\n        num_classes = 10\n        if new_model:\n            self.W = 0.001 * np.random.randn(feat_dim, num_classes)\n        loss_history = []\n        for i in range(num_iters):\n            batch_indices = np.random.choice(num_train, batch_size, replace=True)\n            X_batch = X[batch_indices]\n            Y_batch = Y[batch_indices]\n            if reg_type == 'L1':\n                loss, grad = self.l1_loss(X_batch, Y_batch, reg)\n            else:\n                loss, grad = self.l2_loss(X_batch, Y_batch, reg)\n            loss_history.append(loss)\n            \n            #TODO: update W\n            self.W -= learning_rate * grad\n            \n            #End of your code\n       \n            if display and i % 100 == 0:\n                print(\"In iteration {}/{} , the loss is {}\".format(i, num_iters, loss))\n        if display :\n            print(self.W)\n        return loss_history\n        \n    def calc_loss_grad(self, X, Y):\n        m = X.shape[0]\n        A = softmax(np.dot(X, self.W))\n        # expand the Y\n        Y_ = np.zeros((X.shape[0], self.W.shape[1]))\n        Y_[np.arange(X.shape[0]), Y] = 1\n        J = -1 / m * np.sum(Y_*np.log(A))\n        dw = -1 / m * np.dot(X.T, (Y_-A))\n        return J, dw\n\n    def l1_loss(self, X, Y, reg):\n        # loss = 0\n        # grad = None\n    \n        #TODO: Calculate the loss and gradient\n        loss, grad = self.calc_loss_grad(X, Y)\n        loss = loss + reg * np.sum(abs(self.W))\n        grad = grad + reg\n        \n        # End of your code\n    \n        return loss, grad\n    \n    def l2_loss(self, X, Y, reg):\n        # loss = 0\n        # grad = None\n        \n        #TODO: Calculate the loss and gradient\n        loss, grad = self.calc_loss_grad(X, Y)\n        loss = loss + reg * np.sum(self.W*self.W)\n        grad = grad + 2 * reg * self.W\n        # print(loss, grad)\n        \n        # End of your code\n        \n        return loss, grad\n\n    def predict(self, X):\n        # Y_pred = np.zeros(X.shape[0])\n\n        #TODO: Calculate the prediction\n        Y_ = softmax(np.dot(X,self.W))\n        max_index = np.argmax(Y_, axis = 1)\n        Y_pred = max_index\n        # print(Y_pred)\n        #End of your code\n    \n        return Y_pred\n        \n    def draw_W(self):\n        Xmin = np.min(self.W)\n        Xmax = np.max(self.W)\n        plt.hist(self.W,bins=50)\n        plt.xlabel('Area')\n        plt.xlim(Xmin, Xmax)\n        plt.ylabel('Number')\n        # plt.ylim(0,100)\n        plt.title('W')\n        plt.show()\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"357C61F2396B4616979B175BBDD0DE12","mdEditEnable":false},"source":"### 2.3 训练模型样例\n在这一部分，你不需要完成任何代码，你可以通过这一部分验证你上面实现的LogisticRegression1的代码是否实现正确。"},{"cell_type":"code","execution_count":75,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"BF68315494B84FB2B401C86CB8788A4A","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"In iteration 0/2000 , the loss is 3.2323281955687992\nIn iteration 100/2000 , the loss is 1.0594463210560192\nIn iteration 200/2000 , the loss is 0.6874427680540208\nIn iteration 300/2000 , the loss is 0.5513659178329648\nIn iteration 400/2000 , the loss is 0.5450278846430155\nIn iteration 500/2000 , the loss is 0.6171831804741016\nIn iteration 600/2000 , the loss is 0.6017910654986751\nIn iteration 700/2000 , the loss is 0.35459624822015273\nIn iteration 800/2000 , the loss is 0.4138831578860036\nIn iteration 900/2000 , the loss is 0.3435582489815632\nIn iteration 1000/2000 , the loss is 0.39111284805454793\nIn iteration 1100/2000 , the loss is 0.5267107269526045\nIn iteration 1200/2000 , the loss is 0.3814061947092372\nIn iteration 1300/2000 , the loss is 0.3501531322019915\nIn iteration 1400/2000 , the loss is 0.40526082029707644\nIn iteration 1500/2000 , the loss is 0.3275881127016504\nIn iteration 1600/2000 , the loss is 0.3226397983707609\nIn iteration 1700/2000 , the loss is 0.31426820704635733\nIn iteration 1800/2000 , the loss is 0.3213963870671668\nIn iteration 1900/2000 , the loss is 0.4048492735567795\n[[-1.52845207e-03 -4.26599145e-05 -1.16819544e-03 ...  3.90363757e-04\n  -1.07946478e-03 -4.71030154e-04]\n [ 9.00077971e-04 -1.12011086e-03  1.49130023e-04 ...  5.36179940e-04\n   9.63258484e-04  3.68402833e-04]\n [-4.73147033e-04 -1.30780197e-03  5.37233972e-04 ...  1.22636725e-03\n  -7.23773808e-04  2.25824479e-03]\n ...\n [-6.79239525e-05 -7.36634194e-05  2.66692808e-04 ... -1.53153131e-03\n  -1.89318930e-04  7.67402480e-04]\n [ 6.21549824e-04 -5.03717756e-04  2.91320469e-04 ... -1.94400431e-03\n   1.90574561e-03 -6.51561183e-05]\n [ 8.15110007e-04 -1.75617459e-03 -8.33665573e-04 ... -1.46481451e-03\n   2.92640507e-03 -6.17386834e-04]]\nThe Accuracy is 0.8933\n\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q7g76v1nub.png\">"},"transient":{}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/BF68315494B84FB2B401C86CB8788A4A/q7g76vr5zy.png\">"},"transient":{}}],"source":"lr_param = 1e-6\nreg_param = 0.01\nmodel = LinearRegression2()\nloss_history = model.train(X_train, Y_train, lr_param, reg_param, 'L2')\npred = model.predict(X_test)\nacc = np.mean(pred == Y_test)\nprint(\"The Accuracy is {}\\n\".format(acc))\n\nmodel.draw_W()\n\nx = range(len(loss_history))\nplt.plot(x, loss_history, label='Loss')\nplt.legend()\nplt.xlabel('Iteration Num')\nplt.ylabel('Loss')\nplt.show()"},{"metadata":{"id":"729A6A93708047EE8A92A46AEB87CCA7","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"### 2.4 学习率与损失函数、模型性能的关系\n因为学习率和正则化参数都是超参数，在一般的训练过程中，我们没办法直接优化，所以我们一般会将训练集细分成训练集和验证集，然后通过模型在验证集上的表现选择一个最优的超参数，再将它对应的最优的模型应用到测试集中。\n在这一部分你需要完成以下内容：\n1. 尝试多种不同的学习率\n2. 储存学习率对应的损失函数值到L1_loss和L2_loss中（我们对损失函数值进行了20步平均化处理）。\n3. 储存学习率对应的**在验证集上**的正确率到L1_lr_val_acc和L2_lr_val_acc中\n\n#### 注意：\n因为已有代码中L1_loss，L1_lr_val_acc都是数组，在可视化的过程中我们需要学习率和它们相对应，比如learning_rates[0]对应的loss和validation accuracy应该储存在数组index为0的位置\n\n#### 拓展：\n在这个部分中采取的损失函数都是定值，如果你有时间的话，可以尝试根据迭代轮数改变学习率，并比较不变的学习率和变化的学习率对于模型性能的影响。"},{"metadata":{"id":"5A6CB337C471493B8DE3A3F868E49186","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"L1 7e-05 0.9039166666666667\nL1 5e-05 0.9075\nL1 3e-05 0.9126666666666666\nL1 1e-05 0.9121666666666667\nL1 7e-06 0.9116666666666666\nL1 5e-06 0.9081666666666667\nL1 3e-06 0.903\nL1 1e-06 0.88975\nL1 7e-07 0.8829166666666667\nL2 7e-05 0.9021666666666667\nL2 5e-05 0.9126666666666666\nL2 3e-05 0.912\nL2 1e-05 0.9114166666666667\nL2 7e-06 0.9101666666666667\nL2 5e-06 0.9091666666666667\nL2 3e-06 0.90525\nL2 1e-06 0.8889166666666667\nL2 7e-07 0.8838333333333334\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/5A6CB337C471493B8DE3A3F868E49186/q7g5rdir99.png\">"},"transient":{}},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/5A6CB337C471493B8DE3A3F868E49186/q7g5rd4kf6.png\">"},"transient":{}}],"source":"reg = 0.01\nreg_types = ['L1', 'L2']\nL1_loss = []\nL2_loss = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nL1_lr_val_acc = []\nL2_lr_val_acc = []\n\n#TODO\nlearning_rates = [7e-5, 5e-5, 3e-5, 1e-5, 7e-6, 5e-6, 3e-6, 1e-6, 7e-7]\n\nfor reg_type in range(len(reg_types)):\n    for i in range(len(learning_rates)):\n        model = LinearRegression2()\n        learning_rate = learning_rates[i]\n        loss_history = model.train(X_train, Y_train, learning_rate, reg, reg_types[reg_type], display = False)\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n        \n        if reg_type == 0:\n            L1_loss.append(loss_history)\n            L1_lr_val_acc.append(accuracy)\n        else:\n            L2_loss.append(loss_history)\n            L2_lr_val_acc.append(accuracy)\n            \n        print(reg_types[reg_type], learning_rate, accuracy)\n\n#End of your code\n\n#visulize the relationship between lr and loss\nfor i, lr in enumerate(learning_rates):\n    L1_loss_label = str(lr) + 'L1'\n    L2_loss_label = str(lr) + 'L2'\n    L1_loss_i = L1_loss[i]\n    L2_loss_i = L2_loss[i]\n    ave_L1_loss = np.zeros_like(L1_loss_i)\n    ave_L2_loss = np.zeros_like(L2_loss_i)\n    ave_step = 20\n    for j in range(len(L1_loss_i)):\n        if j < ave_step:\n            ave_L1_loss[j] = np.mean(L1_loss_i[0: j + 1])\n            ave_L2_loss[j] = np.mean(L2_loss_i[0: j + 1])\n        else:\n            ave_L1_loss[j] = np.mean(L1_loss_i[j - ave_step + 1: j + 1])    \n            ave_L2_loss[j] = np.mean(L2_loss_i[j - ave_step + 1: j + 1])\n    x = range(len(L1_loss_i))\n    plt.plot(x, ave_L1_loss, label=L1_loss_label)\n    plt.plot(x, ave_L2_loss, label=L2_loss_label)\n    \nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Loss')\nplt.show()\n\n#visulize the relationship between lr and accuracy\nx = range(len(learning_rates))\nplt.plot(x, L1_lr_val_acc, label='L1_val_acc')\nplt.plot(x, L2_lr_val_acc, label='L2_val_acc')\nplt.xticks(x, learning_rates)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter lr')\nplt.ylabel('Validation Accuracy')\nplt.show()","execution_count":56},{"metadata":{"id":"73FEF9CE79564E738DDA0EF6955113BF","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"mdEditEnable":false},"cell_type":"markdown","source":"#### 拓展\n以下代码尝试了根据迭代轮数改变学习率.\n变化的学习率可以一定程度上做到比不变的学习率的准确性更高，并且这种变化在多分类中比二分类中更为明显。同时，不同阶段选取的学习率对最后的准确性也有较大的影响。最后经过多次尝试，选择了这一组准确性最高的学习率。"},{"metadata":{"id":"A55B40D640D44DF0875F3FD2AB4A9643","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":true},"cell_type":"code","outputs":[{"output_type":"stream","text":"accuracy_altr now is 0.9118333333333334\naccuracy_altr now is 0.9225833333333333\naccuracy_altr now is 0.9209166666666667\naccuracy_altr now is 0.9241666666666667\nTotal accuracy is 0.9241666666666667\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/A55B40D640D44DF0875F3FD2AB4A9643/q7g5uzwnrs.png\">"},"transient":{}}],"source":"reg = 0.01\nL2_altr_loss = []\nlearning_rates_step = [5e-5, 7e-6, 3e-5, 1e-5]\nnum_iter = 500\nmodel = LinearRegression2()\nfor i in range(len(learning_rates_step)):\n    learning_rate = learning_rates_step[i]\n    if i == 0:\n        L2_altr_loss += model.train(X_train, Y_train, learning_rate, reg, 'L2',\n            num_iters = num_iter, display = False, new_model = True)\n    else:\n        L2_altr_loss += model.train(X_train, Y_train, learning_rate, reg, 'L2',\n            num_iters = num_iter, display = False, new_model = False)\n\n    accuracy_altr = np.mean(model.predict(X_val) == Y_val)\n    print(\"accuracy_altr now is {}\".format(accuracy_altr))\n    \naccuracy = np.mean(model.predict(X_val) == Y_val)\nprint(\"Total accuracy is {}\".format(accuracy))\n#visulize l2_altr_loss\nx = range(len(L2_altr_loss))\nplt.plot(x, L2_altr_loss, label='Loss')\nplt.legend()\nplt.xlabel('Iteration Num')\nplt.ylabel('Loss')\nplt.show()","execution_count":64},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"4A4B01F04B1241E089EE29390FDE9D2D","mdEditEnable":false},"source":"### 2.5 正则项与模型性能\n在这一部分中，你需要完成以下内容：\n1. 尝试多个正则化参数的值\n2. 储存对应的在**验证集上**的正确率到L1_reg_val_acc和L2_reg_val_acc中\n3. 通过验证集X_val和Y_val选择最优的正则化超参数，并储存最优正则化参数和对应模型\n\n已有的代码会画出正则化参数和验证集上正确率的关系图，并计算最优的模型在测试集上的正确率。\n\n#### 注意：\n和上面学习率一样，L1_reg_val_acc的存储也需要和正则化参数值对应。"},{"cell_type":"code","execution_count":12,"metadata":{"scrolled":false,"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A37EA8B9B3BE427EA3B674AFC033EB84","collapsed":false},"outputs":[{"output_type":"stream","text":"L1 0.001 0.8933333333333333\nL1 0.005 0.8933333333333333\nL1 0.01 0.8940625\nL1 0.015 0.8958333333333334\nL1 0.02 0.89625\nL1 0.025 0.8923958333333334\nL1 0.03 0.8946875\nL1 0.035 0.895\nL1 0.04 0.8959375\nL2 0.001 0.8925\nL2 0.005 0.8934375\nL2 0.01 0.8945833333333333\nL2 0.015 0.895\nL2 0.02 0.8934375\nL2 0.025 0.8960416666666666\nL2 0.03 0.8971875\nL2 0.035 0.8947916666666667\nL2 0.04 0.8944791666666667\n","name":"stdout"},{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/A37EA8B9B3BE427EA3B674AFC033EB84/q7myuafmv2.png\">"},"transient":{}},{"output_type":"stream","text":"The Accuracy with L1 regularization parameter 0.02 is 0.9001\n\nThe Accuracy with L2 regularization parameter 0.03 is 0.8995\n\n","name":"stdout"}],"source":"learning_rate = 1.5e-6\nreg_types = ['L1', 'L2']\nL1_reg_val_acc = []\nL2_reg_val_acc = []\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.2)\nbest_L1_model = None\nbest_L2_model = None\nbest_L1_reg = 0\nbest_L2_reg = 0\n\n#TODO\nregs = [0.001, 0.005, 0.01, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04]\nfor reg_type in range(len(reg_types)):\n    best_accuracy = 0\n    for i in range(len(regs)):\n        model = LinearRegression2()\n        reg = regs[i]\n        loss_history = model.train(X_train, Y_train, learning_rate, reg, reg_types[reg_type], display = False)\n        accuracy = np.mean(model.predict(X_val) == Y_val)\n\n        if reg_type == 0:\n            L1_reg_val_acc.append(accuracy)\n            if (accuracy > best_accuracy):\n                best_accuracy = accuracy\n                best_L1_model = model\n                best_L1_reg = reg\n        else:\n            L2_reg_val_acc.append(accuracy)\n            if (accuracy > best_accuracy):\n                best_accuracy = accuracy\n                best_L2_model = model\n                best_L2_reg = reg\n        print(reg_types[reg_type], reg, accuracy)\n\n#End of your code\n\n\n#visuliza the relation of regularization parameter and validation accuracy\nx = range(len(regs))\nplt.plot(x, L1_reg_val_acc, label='L1_val_acc')\nplt.plot(x, L2_reg_val_acc, label='L2_val_acc')\nplt.xticks(x, regs)\nplt.margins(0.08)\nplt.legend()\nplt.xlabel('high-parameter reg')\nplt.ylabel('Validation Accuracy')\nplt.show()\n\n#Compute the performance of best model on the test set\nL1_pred = best_L1_model.predict(X_test)\nL1_acc = np.mean(L1_pred == Y_test)\nprint(\"The Accuracy with L1 regularization parameter {} is {}\\n\".format(best_L1_reg, L1_acc))\nL2_pred = best_L2_model.predict(X_test)\nL2_acc = np.mean(L2_pred == Y_test)\nprint(\"The Accuracy with L2 regularization parameter {} is {}\\n\".format(best_L2_reg, L2_acc))"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"EBA02A5D4B2147B583047207362FCAEC","mdEditEnable":false},"source":"#### Question2: 对于上面的多分类逻辑回归模型，你觉得它的权重矩阵数值上会呈现出什么样子？你可以通过可视化的方法观察权重矩阵。\nAnswer2:权重矩阵数值的绝对值都较小，绝对值中最大的大概在$10^{-3}$这一量级。根据**1.3**和**2.3**部分代码对W矩阵数值分析后显示的直方图，其分布于sigmoid函数的导函数图像类似，可见softmax和sigmoid函数对W矩阵数值分布的影响。"},{"metadata":{"id":"DC024FC76A4C4F0CABB9B2A2A40F04CB","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}