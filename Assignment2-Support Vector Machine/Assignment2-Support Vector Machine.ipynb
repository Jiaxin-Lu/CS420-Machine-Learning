{"cells":[{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"DC2E6D77D61947D98468860C12ABEB2F","mdEditEnable":false},"source":"# 支持向量机（Support Vector Machine）\n\n## 简介\n\n支持向量机的原始算法于1963年由苏联科学家 **弗拉基米尔·万普尼克(Владимир Наумович Вапник)** 等发明，最初发表于俄文期刊 **自动化与远程控制 (АВТОМАТИКА И ТЕЛЕМЕХАНИКА)**. \n\n点击 [Узнавание образов при помощи обобщенных портретов](http://www.mathnet.ru/links/15c09987c4667b628d040141610ef1b6/at11885.pdf) 查看原始俄语论文，也可以查看英语翻译版 [Recognition of Patterns with help of Generalized Portraits](http://web.cs.iastate.edu/~cs573x/vapnik-portraits1963.pdf)\n\n直到1990年代，由于核技巧(kernel trick)的引入，使得SVM有了非线性的分类能力，SVM才开始获得成功，并引起了一次人工智能的高潮。\n\n在这次作业中，你们需要实现:\n- 最基本的线性支持向量机\n- 梯度下降的优化算法\n- 序列最小优化(SMO)算法和核技巧\n- 学会使用scikit-learn中的SVM\n\n并回答一些相关问题"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"836CE24CB23B47259BB5AF90C2B5D437","mdEditEnable":false},"source":"## 线性 SVM 分类器与梯度下降\n\n在这一部分中，你们需要实现最基本的线性SVM分类器，并完成一个二分类问题，首先我们需要制造一点数据。运行下面的代码就可以得到一个简单的数据集。"},{"cell_type":"code","execution_count":24,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F42D6DCD46B3414A9BAA4EEFAD7EA806","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/F42D6DCD46B3414A9BAA4EEFAD7EA806/q8l04czg0z.png\">"},"transient":{}}],"source":"import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(1024)\n\ndef data_visualization(x, y, pdim=2):\n    category = {'+1': [], '-1': []}\n    for point, label in zip(x, y):\n        if label == 1.0: category['+1'].append(point)\n        else: category['-1'].append(point)\n    fig = plt.figure()\n    if pdim == 2 or pdim == 1:\n        ax = fig.add_subplot(111)\n    elif pdim == 3:\n        ax = fig.add_subplot(111, projection='3d')\n\n    for label, pts in category.items():\n        pts = np.array(pts)\n        if pdim == 1:\n            ax.scatter(pts[:, 0], label=label)\n        elif pdim == 2:\n            ax.scatter(pts[:, 0], pts[:, 1], label=label)\n        elif pdim == 3:\n            if label == '+1':\n                c = 'blue'\n                m = 'o'\n            else:\n                c = 'black'\n                m = 'x'\n            ax.scatter(pts[:, 0], pts[:, 1], pts[:, 2], c=c, marker=m)\n    plt.show() \n\n\n# random a dataset on 2D plane\ndef simple_synthetic_data(n, n0=5, n1=5): # n: number of points, n0 & n1: number of points on boundary\n    # random a line on the plane\n    w = np.random.rand(2) \n    w = w / np.sqrt(w.dot(w))\n    \n    # random n points \n    x = np.random.rand(n, 2) * 2 - 1\n    d = (np.random.rand(n) + 1) * np.random.choice([-1,1],n,replace=True) # random distance from point to the decision line, d in [-2,-1] or [1,2]. d=-1 or d=1 indicate the boundary in svm\n    d[:n0] = -1\n    d[n0:n0+n1] = 1\n    \n    # shift x[i] to make the distance between x[i] and the decision become d[i]\n    x = x - x.dot(w).reshape(-1,1) * w.reshape(1,2) + d.reshape(-1,1) * w.reshape(1,2)\n    \n    # create labels\n    y = np.zeros(n)\n    y[d < 0] = -1\n    y[d >= 0] = 1\n    return x, y\n\nx_data, y_data = simple_synthetic_data(200)\ndata_visualization(x_data, y_data)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C9A17F0311664EC895949B8F9BD2B80E","mdEditEnable":false},"source":"### 线性SVM\n\n对于一组二分类数据,如下图：\n\n\n![Image Name](https://cdn.kesci.com/upload/image/q7l1vpc602.png?imageView2/0/w/480/h/480)\n\n\n在课堂上，我们首先引入间隔函数: \n\n$$\n\\hat{\\gamma}^{(i)}=y^{(i)}(\\boldsymbol{w^\\top}\\boldsymbol{x}^{(i)} + b)\n$$\n\n其中，$\\boldsymbol{w}$为分界(超)平面的一个法向量，$\\boldsymbol{x}^{(i)}$ 为第$i$个样本，$\\boldsymbol{y}^{(i)}$ 为第$i$个样本的标签，我们很容易看出$\\boldsymbol{w^\\top}\\boldsymbol{x}^{(i)} + b$类似于一个 **没有取绝对值的点到平面的距离公式** ，$\\boldsymbol{y}^{(i)}$则指示了在分界平面之上或者之下，即样本点的类别。我们希望**所有点到分类边界的距离越远越好**，此外，在运算中，将法向量归一化是一个很合理的想法，于是，问题转化为\n\n$$\n\\begin{aligned}\n & \\max_{\\gamma,\\boldsymbol{w},b} \\gamma \\\\\\\\\n\\mathrm{s.t.}\\quad & y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge {\\gamma},\\quad i=1..,m\\\\\n & \\|\\boldsymbol{w}\\|_2 = 1\n\\end{aligned}\n$$\n\n其中$\\gamma = \\min\\{\\hat{\\gamma}^{(1)}, \\hat{\\gamma}^{(1)}, \\hat{\\gamma}^{(1)}, \\cdots, \\hat{\\gamma}^{(n)}\\}$，这个问题等价于\n\n$$\n\\begin{aligned}\n & \\max_{\\gamma,\\boldsymbol{w},b} \\frac{\\gamma}{\\|\\boldsymbol{w}\\|_2} \\\\\\\\\n\\mathrm{s.t.}\\quad & y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge {\\gamma},\\quad i=1..,m\\\\\n\\end{aligned}\n$$\n\n我们可以规定数据点最小的距离为1，则\n\n$$\n\\begin{aligned}\n & \\max_{\\boldsymbol{w},b} \\frac{1}{\\|\\boldsymbol{w}\\|_2} \\equiv \\min_{\\boldsymbol{w},b} \\frac{1}{2}\\|\\boldsymbol{w}\\|_2^2\\\\\\\\\n\\mathrm{s.t.}\\quad & y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge 1,\\quad i=1..,m\\\\\n\\end{aligned}\n$$\n\n\n这样一来，我们就把这个问题从一个关于$\\gamma,w,b$非凸优化转化为一个关于$w, b$的凸优化。\n\n所以我们可以通过之前讲过的梯度下降方法来求解这个问题，即优化一个损失函数。我们在课上学习了Hinge Loss，对于一组数据，可以写成\n\n$$\n{\\cal{L}}_{\\mathrm{SVM}} = C\\|\\boldsymbol{w}\\|_2^2 + \\sum_{i=1}^{N}\\max(0, 1 - y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b))\n$$\n\n其中 $C$ 为可调节的常数，作为 L2 正则化系数\n\n下面请回答这样几个问题，请使用markdown语法直接在题目下方作答："},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"85A7EF9EE0D145188AA65C1F57219778","mdEditEnable":false},"source":"1. 为什么最小化Hinge Loss等价于优化上面关于$\\boldsymbol{w},b$的优化问题？（提示：不必严格证明，只需简单合理的说明）\n\n满足 $y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b)\\ge 1$ 时，有 ${\\cal{L}}_{\\mathrm{SVM}} = C\\|\\boldsymbol{w}\\|_2^2 $ 故此时最小化Hinge Loss即是满足了\n$\\min_{\\boldsymbol{w},b} \\frac{1}{2}\\|\\boldsymbol{w}\\|_2^2$。\n\n故此时最小化Hinge Loss等价于优化关于 $\\boldsymbol{w},b$ 的优化问题。\n\n而若 $\\exists i, y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) < 1$, 则 $1 - y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b)$ 就会作为一个增长率足够快（即其导数较 $C\\|\\boldsymbol{w}\\|_2^2$ 这一项足够大）的惩罚被计算到损失函数中。因此能够导致此时能够取到的 ${\\cal{L}}_{\\mathrm{SVM}}$ 并不是最小值，进而促使其继续变小向前一种情况靠拢，进而达到满足关于$\\boldsymbol{w},b$的优化问题中的条件。\n\n综上，最小化Hinge Loss可以保证在满足 $y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b)\\ge 1$ 的条件下取到$\\min_{\\boldsymbol{w},b} \\frac{1}{2}\\|\\boldsymbol{w}\\|_2^2$， 即满足关于$\\boldsymbol{w},b$的优化问题。\n    \n\n2. 证明 $f(\\boldsymbol{w}, b) = C\\|\\boldsymbol{w}\\|_2^2 + \\max(0, 1 - y(\\boldsymbol{w}^\\top \\boldsymbol{x} + b))$ 是凸函数（提示：请复习凸函数的定义，你可以利用任何在先修课程中学过的性质、方法和技巧来证明，比如可以通过函数复合的性质来证明，合理即可）\n\t\n我们显然可以通过图像知道 $g(l) = \\max(0, 1-l)$ 为一个凸函数。\n令 $z = y(\\boldsymbol{w}^\\top \\boldsymbol{x} + b)$，有$z$为一个仿射函数。\n由于凸函数与仿射函数符合保留凸性，故有 $g(z)$ 为凸函数。\n再考虑 $h(\\boldsymbol{w}) = C\\|\\boldsymbol{w}\\|_2^2$ 。\n对于 $\\forall t \\in [0, 1], \\boldsymbol{w_1}, \\boldsymbol{w_2}$,我们有\n$$\n\\begin{align*}\n\\| t\\boldsymbol{w_1} + (1 - t)\\boldsymbol{w_2}\\|_2^2 &\\leq\n\\|t\\boldsymbol{w_1}\\|_2^2 + \\|(1 - t)\\boldsymbol{w_2}\\|_2^2 \\\\\n&= t\\|\\boldsymbol{w_1}\\|_2^2 + (1 - t)\\|\\boldsymbol{w_2}\\|_2^2.\n\\end{align*}\n$$\n因此 $\\|\\boldsymbol{w}\\|_2^2$ 是凸函数，显然 $C\\|\\boldsymbol{w}\\|_2^2$ 亦为凸函数。\n而 $f(\\boldsymbol{w}, b) = h(\\boldsymbol{w}) + g(z)$， 两个凸函数之和为凸函数，故 $f(\\boldsymbol{w}, b)$ 为凸函数。\n    \n    \n\n3. 对于一个凸函数 $f(\\boldsymbol{x})$，假设我们找到了一个局部最优解 $\\boldsymbol{x_0}$，证明这个最优解是全局最优解。（提示：凸函数定义）\n假设在全集S上存在一个点$x'$，使得\n$$\nf(x') < f(\\boldsymbol{x_0})\n$$\n由于f(x)是凸函数，所以对于$\\forall t \\in [0, 1]$,有\n$$\nf(tx'+(1−t)\\boldsymbol{x_0})≤tf(x')+(1−t)f(\\boldsymbol{x_0})\n$$\n当$t \\rightarrow 0$，此时 $tx' + (1-t)\\boldsymbol{x_0} \\rightarrow \\boldsymbol{x_0}$ 而 $f(tx' + (1-t)\\boldsymbol{x_0}) < f(\\boldsymbol{x_0})$，则有 $f(\\boldsymbol{x_0})$ 非局部最优解，矛盾。\n故$x'$不存在，即局部最优解$\\boldsymbol{x_0}$是全局最优解。\n    "},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F281792FE34441858214263B6C227A2D","mdEditEnable":false},"source":"如果你成功回答了上面的问题，你就可以理解我们可以通过梯度下降的方式找到SVM的最优解，下面就需要实现基本的SVM以及梯度下降"},{"cell_type":"code","execution_count":31,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"037C9A6795484CB68A4E27BCEAFFD019","collapsed":false,"scrolled":false},"outputs":[],"source":"def svm_sgd(w, b, X_in, y_in, reg):\n    \"\"\"\n    Inputs have dimension D, we operate on minibatches of N examples.\n    - w: The weight, a numpy array of shape (D,) containing weights \n    - b: The bias, a constant \n    - X_in: A numpy array of shape (N, D) containing a minibatch of data.\n    - y_in: A numpy array of shape (N,) containing training labels; y[i] = -1/ 1 means that X[i] has label -1/1\n    - reg: regularization factor, constant, float\n\n    Returns a tuple of:\n    - loss, loss as single float\n    - dw, gradient with respect to weights W; an array of same shape as W\n    - db, gradient with respect to bias b, a constant\n    \"\"\"\n    loss = 0.0\n    dw = np.zeros(w.shape)\n    db = 0.0\n    #############################################################################\n    # TODO:                                                                     #\n    # Implement SVM loss, storing the result in loss.                           #\n    # **No explicit loop allowed, otherwise your points will be deducted**      #\n    #############################################################################\n    N, D = X_in.shape\n    \n    dis = 1 - y_in * (np.dot(X_in, w) + b)\n    dis = np.maximum(dis, 0)\n    hinge_loss = np.sum(dis)\n    loss += reg * np.dot(w,w) + hinge_loss\n    \n    #############################################################################\n    #                             END OF YOUR CODE                              #\n    #############################################################################\n    \n    #############################################################################\n    # TODO:                                                                     #\n    # Implement SVM gradient respect to w and b,                                #\n    # storing the result in dw, db, respectively.                               #\n    # **No explicit loop allowed, otherwise your points will be deducted**      #\n    # **Always assume that the loss function is differentiable**                #\n    #############################################################################\n    \n    y_ = np.sign(dis) * y_in\n    dw = 2 * reg * w - np.dot(np.transpose(X_in), y_)\n    db = -np.sum(y_)\n    \n    #############################################################################\n    #                             END OF YOUR CODE                              #\n    #############################################################################\n    return loss, dw, db"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"93BF47C3F42B47ACB1046C507964BBB3","mdEditEnable":false},"source":"到这里你们已经完成了基本的线性SVM，及其梯度计算，下面的一些代码可以用于检查调试你的代码"},{"cell_type":"code","execution_count":32,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"8A52B96678B149E98021C2DD4564635C","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"#0 randomly calculate svm loss: 68.414335, relative dw error: 0.000000000001, relative db error: 0.000000000012\n-34.0 -34.000000000844466\n-151.19569258510782 -151.1956925853042\n#1 randomly calculate svm loss: 513.021816, relative dw error: 0.000000000016, relative db error: 0.000000000159\n-4.0 -4.000000001269655\n-244.7799637403601 -244.77996374798747\n#2 randomly calculate svm loss: 238.745053, relative dw error: 0.000000000002, relative db error: 0.000000000159\n-4.0 -4.000000001269655\n-243.85857085225166 -243.85857085320592\n#3 randomly calculate svm loss: 256.918632, relative dw error: 0.000000000011, relative db error: 0.000000000008\n-102.0 -101.99999999827013\n-121.3292313481044 -121.32923135084182\n#4 randomly calculate svm loss: 292.255738, relative dw error: 0.000000000003, relative db error: 0.000000000023\n-39.0 -38.99999999816828\n-120.74148748757212 -120.74148748695278\n#5 randomly calculate svm loss: 236.846738, relative dw error: 0.000000000004, relative db error: 0.000000000035\n-22.0 -21.999999998456584\n-206.20183893808255 -206.20183893953478\n#6 randomly calculate svm loss: 96.232410, relative dw error: 0.000000000001, relative db error: 0.000000000004\n-35.0 -34.999999999740794\n-126.03782184435902 -126.03782184470445\n#7 randomly calculate svm loss: 91.373732, relative dw error: 0.000000000002, relative db error: 0.000000000004\n94.0 93.99999999928353\n-120.62296478606028 -120.6229647856105\n#8 randomly calculate svm loss: 546.603635, relative dw error: 0.000000000000, relative db error: 0.000000000159\n12.0 12.000000003808962\n-236.275652610895 -236.27565261108427\n#9 randomly calculate svm loss: 222.657342, relative dw error: 0.000000000001, relative db error: 0.000000000015\n-41.0 -40.999999998803105\n-151.78770614859917 -151.78770614880932\n","name":"stdout"}],"source":"from random import randrange, seed\n\nseed(1024)\ndef check_gradient_simple():\n    \"\"\"\n    This function check gradient using numerical method, formally:\n    \\frac{\\partial f}{\\partial w_i} = (f(w_i + h) - f(w_i - h)) / 2h\n    The relative error should be very small\n    \"\"\"\n    reg = 0.5\n    for i in range(10):\n        w = np.random.randn(2) \n        b = np.random.randn(1)\n        loss, dw, db = svm_sgd(w, b, x_data, y_data, reg)\n        h = 1e-5\n        idx = randrange(2)\n        \n        wp = w.copy()\n        wp[idx] += h\n        \n        wm = w.copy()\n        wm[idx] -= h\n        \n        evalwp, _, _ = svm_sgd(wp, b, x_data, y_data, reg)\n        evalwm, _, _ = svm_sgd(wm, b, x_data, y_data, reg)\n        \n        grad_num_w = (evalwp - evalwm) / (2 * h)\n        \n        bp = b + h\n        bm = b - h\n        evalbp, _, _ = svm_sgd(w, bp, x_data, y_data, reg)\n        evalbm, _, _ = svm_sgd(w, bm, x_data, y_data, reg)\n        grad_num_b = (evalbp - evalbm) / (2 * h)\n        \n        rel_dw_error = np.abs(dw[idx] - grad_num_w) / (np.abs(dw[idx]) + np.abs(grad_num_w))\n        rel_db_error = np.abs(db - grad_num_b) / (np.abs(db) + np.abs(grad_num_b))\n        \n        print('#%d randomly calculate svm loss: %f, relative dw error: %.12f, relative db error: %.12f'\n              % (i, loss, rel_dw_error, rel_db_error))\n              \n        print(db, grad_num_b)\n        print(dw[idx], grad_num_w)\n        \ncheck_gradient_simple()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D8CE2B66F63A47539A7AB98F62A7BE03","mdEditEnable":false},"source":"如果你的相对误差大多数量级在 $10^{-10}$ 及以下，说明你通过了梯度检查，说明你的代码实现正确。然后请你回答以下问题：\n\n4. 在计算梯度时，我们假设函数可微，但事实上并非如此，如果你运行梯度检查足够多次，你会发现梯度检查可能失败，请说明在何种情况下我们无法计算梯度（简要说明，无需严格证明，合理即可）\n\n当 db==0 的时候会出现梯度检查失效，也就是计算出来的 hinge_loss==0 的情况下会计算失效。\n此时有，\n$$\n\\max(0, 1 - y_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b)) = 0\n$$\n也就是 \n$$\ny_i(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)} + b) \\ge 1\n$$\n恒成立\n此时hinge_loss处于图像上恒为 $0$ 的一段，因而出现无法计算梯度的情况。\n\n回答问题之后，就可以开始训练简单的线性SVM分类器了"},{"cell_type":"code","execution_count":8,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"E0CB2CD76C384009A9A539A4707C3306","collapsed":false,"scrolled":false},"outputs":[{"output_type":"stream","text":"epoch 1, the loss is 174.733321284959\nepoch 2, the loss is 0.000000000000\nepoch 3, the loss is 0.000000000000\nepoch 4, the loss is 0.000000000000\nepoch 5, the loss is 0.000000000000\nepoch 6, the loss is 0.000000000000\nepoch 7, the loss is 0.000000000000\nepoch 8, the loss is 0.000000000000\nepoch 9, the loss is 0.000000000000\nepoch 10, the loss is 0.000000000000\n","name":"stdout"}],"source":"def train_svm_sgd(w, b, num_epoch, lr=5e-3, reg=0.0):\n    \"\"\"\n    Implement SVM SGD training algorithm here, we randomly initilaize the weight and bias\n    \"\"\"\n    \n    for e in range(num_epoch):\n        loss, dw, db = svm_sgd(w, b, x_data, y_data, reg)\n        print('epoch %d, the loss is %.12f' % (e+1, loss))\n    #############################################################################\n    # DONE:                                                                     #\n    # Implement Gradient Descent with learning decay to train SVM               #\n    #############################################################################\n    \n        w -= lr * dw\n        b -= lr * db\n    \n    #############################################################################\n    #                             END OF YOUR CODE                              #\n    #############################################################################\n    return w, b\n    \nw = np.random.randn(2)\nb = np.random.randn(1)\nw, b = train_svm_sgd(w, b, 10)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"372EC50050AA407B8C57647A88AEC04E","mdEditEnable":false},"source":"如果一切正常，那么你可以看到损失函数降到了接近0,这是显然的，因为数据是线性可分的，我们可以将训练得到的结果可视化："},{"cell_type":"code","execution_count":5,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5ECAA2857EFF45B88956595DABB51F53","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/5ECAA2857EFF45B88956595DABB51F53/q89le1ytix.png\">"},"transient":{}}],"source":"import matplotlib.pyplot as plt\ncategory = {'+1': [], '-1': []}\nfor point, label in zip(x_data, y_data):\n    if label == 1.0: category['+1'].append(point)\n    else: category['-1'].append(point)\nfig = plt.figure()\nax = fig.add_subplot(111)\n\n# plot points\nfor label, pts in category.items():\n    pts = np.array(pts)\n    ax.scatter(pts[:, 0], pts[:, 1], label=label)\n\n# calculate weight\nweight = w\nbias = b\n# plot the model: wx+b\nx1 = np.min(x_data[:, 0])\ny1 = (-bias - weight[0] * x1) / weight[1]\nx2 = np.max(x_data[:, 0])\ny2 = (-bias - weight[0] * x2) / weight[1]\nax.plot([x1, x2], [y1, y2])\n\n            \nplt.show()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"10CC2C73B1054365B0118207C2E6B50C","mdEditEnable":false},"source":"通过可视化，可以看到我们已经找到了一个分类边界。如果你多次运行上面的训练和可视化过程，可以发现由于**超参数**和**随机性**的存在，尽管理论上最优解存在，但SGD找到的结果并不总是最好的。因为我们希望，对于一组数据点，假设数据点到分类边界的距离的集合为$\\{\\gamma_1,\\gamma_2,\\gamma_3,\\cdots, \\gamma_N\\}$，我们所期望的优化效果是\n$$\n    \\boldsymbol{w}, b = \\arg\\max_{\\boldsymbol{w}, b}\\min\\{\\gamma_1,\\gamma_2,\\gamma_3,\\cdots, \\gamma_N\\}\n$$\n\n而SVM的优化作为一个凸优化，显然有更好的算法来优化。"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D2C4CBB0318C4376850CB0E4424B54A3"},"source":"## 序列最小优化算法(Sequential Minimal Optimization)与核技巧(Kernel Trick)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F32CD830A11648A5AB2B3C3A894D1E8C","mdEditEnable":false},"source":"SMO算法于1998由John Platt发明，点击可以查看原始论文\n\n[Sequential Minimal Optimization:A Fast Algorithm for Training Support Vector Machines](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-98-14.pdf)\n\n这个算法的效率非常高\n\n在课程中，我们已经介绍了我们可以用拉格朗日函数来求解SVM：\n\n$$\n\\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha})=\\frac{1}{2}||\\boldsymbol{w}||^2 - \\sum_{i=1}^{m}\\alpha_i[y^{(i)}(\\boldsymbol{w}^\\top \\boldsymbol{x}^{(i)}+b) - 1]\n$$\n\n这个函数依然是个凸函数，因此极值点梯度为0，我们对$\\boldsymbol{w}, b$分别求梯度求解得到\n\n$$\n\\frac{\\partial \\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha})}{\\partial \\boldsymbol{w}} = \\boldsymbol{w}-\\sum_{i=1}^{m}\\alpha_i y^{(i)}\\boldsymbol{x}^{(i)}=0 \\Rightarrow \\boldsymbol{w}=\\sum_{i=1}^{m}\\alpha_i y^{(i)}\\boldsymbol{x}^{(i)} \\\\\n\\frac{\\partial \\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha})}{\\partial b} = \\sum_{i=1}^{y^{(i)}} \\alpha_i y^{(i)}=0\n$$\n\n等量代换得到：\n\n$$\n\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha}) & = \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)}.\n\\end{aligned}\n$$"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B922624CFCED471E9002BE77251F854A","mdEditEnable":false},"source":"由于**强对偶**性，我们把问题转化为\n$$\n\\begin{aligned}\n\\max_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\mathcal{L}(\\boldsymbol{w},b,\\boldsymbol{\\alpha}) = \\max_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} \\\\\n= \\min_{\\alpha: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} - \\sum_{i=1}^{m} \\alpha_i \\\\\n\\mathcal{s.t.} \\qquad \\sum_{i=1}^{y^{(i)}} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad \\alpha_i \\ge 0\n\\end{aligned}\n$$\n\n对于**线性不可分**的情况\n\n$$\n\\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} - \\sum_{i=1}^{m} \\alpha_i \\\\\n\\mathcal{s.t.} \\qquad \\sum_{i=1}^{m} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad 0 \\le \\alpha_i \\le C\n$$\n\n关于强对偶性以及线性不可分的证明可以参考 **课上内容** 或者 吴恩达的笔记 [Support Vector Machine](http://cs229.stanford.edu/notes/cs229-notes3.pdf)，这里不再详细说明\n\n"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"C2A52F022E154E548540EA22B425D0EA","mdEditEnable":false},"source":"于是我们就可以通过SMO算法来求解SVM优化问题，可能课上讲的不够详细，这里给出更具体的参考\n\n[Stanford CS229 2009 SMO](http://cs229.stanford.edu/materials/smo.pdf)\n\n也可以把原始论文作为参考，下面请实现SMO算法"},{"cell_type":"code","execution_count":32,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B457201D23734F01A9D8F4882AD5BD74","collapsed":false,"scrolled":false},"outputs":[],"source":"def default_kernel(x, z):\n    return x.dot(z.T)\n\ndef svm_smo(X_in, y_in, C, max_iter, epsilon=1e-5, kernel=default_kernel, isDefault = True):\n    \"\"\"\n    Inputs have dimension D, we operate on minibatches of N examples.\n    - X_in: A numpy array of shape (N, D) containing a minibatch of data.\n    - y_in: A numpy array of shape (N,) containing training labels; y[i] = -1/1 means that X[i] has label -1/1\n    - kernel: kernel function, using inner product by default\n    - C: relax factor\n    - max_iter: max iteration\n    - epsilon: numerical tolerance\n\n    Returns a tuple of:\n    - alpha: Described as above, a np array of shape (N, )\n    - b: the bias\n    \"\"\"\n    n, _ = X_in.shape\n    alpha = np.zeros((n,))\n    b = 0\n    \n    #############################################################################\n    # TODO:                                                                     #\n    # Implement SMO Algorithm                                                   #\n    #############################################################################\n    def clipAlpha(al, high, low):\n        if al < low :\n            return low\n        if al > high :\n            return high\n        return al\n    \n    def selectRandJ(i, n):\n        j = np.random.randint(low = 0, high = n-1)\n        while (i == j):\n            j = np.random.randint(low = 0, high = n-1)\n        return j\n    \n    ker = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            ker[i, j] = kernel(X_in[i], X_in[j])\n    iter = 0\n    while iter < max_iter:\n        alphaChanged = 0\n        for i in range(n) :\n            Ei = np.sum(ker[:, i]*alpha*y_in) + b - y_in[i]\n            if ((y_in[i]*Ei < -epsilon) and (alpha[i]<C)) or \\\n                ((y_in[i]*Ei > epsilon) and (alpha[i] > 0)):\n                j = selectRandJ(i, n)\n                Ej = np.sum(ker[:,j]*alpha*y_in) + b - y_in[j]\n                alpha_i = alpha[i].copy()\n                alpha_j = alpha[j].copy()\n                if y_in[i] == y_in[j] :\n                    low = max(0, alpha_j + alpha_i - C)\n                    high = min(C, alpha_i + alpha_j)\n                else :\n                    low = max(0, alpha_j - alpha_i)\n                    high = min(C, C + alpha_j - alpha_i)\n                if low == high : continue\n                eta = 2.0*ker[i,j] - ker[i,i] - ker[j,j]\n                if (eta >= 0): continue\n                alpha[j] = alpha[j] - y_in[j]*(Ei-Ej)/eta\n                alpha[j] = clipAlpha(alpha[j], high, low)\n                if (np.abs(alpha[j] - alpha_j) < 1e-5) : continue\n                alpha[i] = alpha[i] + y_in[i]*y_in[j]*(alpha_j-alpha[j])\n                b1 = b-Ei-y_in[i]*(alpha[i] - alpha_i)*ker[i,i] - y_in[j]*(alpha[j]-alpha_j)*ker[i,j]\n                b2 = b-Ej-y_in[i]*(alpha[i] - alpha_i)*ker[i,j] - y_in[j]*(alpha[j]-alpha_j)*ker[j,j]\n                if (0 < alpha[i] < C) :\n                    b = b1\n                elif (0 < alpha[j] < C):\n                    b = b2\n                else: b = (b1 + b2)/2\n                alphaChanged += 1\n        iter += 1\n    #############################################################################\n    #                             END OF YOUR CODE                              #\n    #############################################################################\n    \n    return alpha, b"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"F768A0ABD5074DE6A3FC7513EB7F719E","mdEditEnable":false},"source":"至此你已经完成了SMO算法，下面的代码会用于测试你的代码"},{"cell_type":"code","execution_count":7,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"529CCCFACD4049DD973C2678F23DC8E6","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/529CCCFACD4049DD973C2678F23DC8E6/q8kw9kguf1.png\">"},"transient":{}}],"source":"def plot_result(x, y, alpha, bias):\n    category = {'+1': [], '-1': []}\n    for point, label in zip(x, y):\n        if label == 1.0: category['+1'].append(point)\n        else: category['-1'].append(point)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # plot points\n    for label, pts in category.items():\n        pts = np.array(pts)\n        ax.scatter(pts[:, 0], pts[:, 1], label=label)\n\n    # calculate weight\n    weight = 0\n    for i in range(alpha.shape[0]):\n        weight += alpha[i] * y[i] * x[i]\n\n    # plot the model: wx+b\n    x1 = np.min(x[:, 0])\n    y1 = (-bias - weight[0] * x1) / weight[1]\n    x2 = np.max(x[:, 0])\n    y2 = (-bias - weight[0] * x2) / weight[1]\n    ax.plot([x1, x2], [y1, y2])\n\n    # plot the support vectors\n    for i, alpha_i in enumerate(alpha):\n        if abs(alpha_i) > 1e-3: \n            ax.scatter([x[i, 0]], [x[i, 1]], s=150, c='none', alpha=0.7,\n                       linewidth=1.5, edgecolor='#AB3319')\n\n    plt.show()\n\nalpha, bias = svm_smo(x_data, y_data, 1e10, 1000)\n\nplot_result(x_data, y_data, alpha, bias)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"3DCBAE0CEBB24087993A5EF3B565EFA0","mdEditEnable":false},"source":"如果一切正常，你会看到一条优秀的分界线，圈出的数据点代表了 **支持向量**，支持向量决定了分界面的位置，直接表现是对应的数据点约束条件的 **拉格朗日乘子系数较大**"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"0B344435F8B34B268A440A7BDF6A81A8"},"source":"至此，我们已经对线性SVM以及其优化有了全面的了解，下面我们引入 **核函数** 的概念\n\n并不是所有的数据都是平面可以分割的，比如下面的数据"},{"cell_type":"code","execution_count":33,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9E81B0BE926C49B08DD1530CF3FC95E9","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/9E81B0BE926C49B08DD1530CF3FC95E9/q8l0f1ezza.png\">"},"transient":{}}],"source":"data = np.loadtxt('/home/kesci/input/svm2640/data.txt')\nx_sp = data[:,:2]\ny_sp = data[:,2]\ndata_visualization(x_sp, y_sp)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"D11AB52761B347308D5414C6F966FECB","mdEditEnable":false},"source":"显而易见，上面的数据很难用一个平面进行划分，但是我们发现，如果我们将数据点映射到高维空间，事情就会发生变化，但是这种映射难以找到，下面给出一个映射的例子，这个映射不能分割数据，大家可以自行尝试其他的映射方式，此部分不计分，如果能够在三维空间中使得数据线性可分，则可以获得额外分。\n$$\n\\phi(\\boldsymbol{x}) = \\begin{bmatrix}\nx_1^2\\\\\nx_2^2\\\\\nx_1 + x_2\\\\\n\\end{bmatrix}\n$$"},{"cell_type":"code","execution_count":9,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"B164EBBE404846B09D70059C7C04D918","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/B164EBBE404846B09D70059C7C04D918/q8kwaa2ymc.png\">"},"transient":{}}],"source":"from mpl_toolkits.mplot3d import Axes3D\nbase = np.exp(20 * ((x_sp[:,:1]*x_sp[:,:1]) + (x_sp[:,1:]*x_sp[:,1:])))\nx_sp_p = np.concatenate([base,\n                         base * x_sp[:, :1],\n                         base * x_sp[:, 1:]],\n                         axis=-1)\n# you can try to modify x_sp_p\ndata_visualization(x_sp_p, y_sp, 3)"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"9883ED2420F9497E801B4CA4BDF5F04D","mdEditEnable":false},"source":"我们已经知道了线性SVM的优化问题：\n$$\n\\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\boldsymbol{x}^{(i)\\top} \\boldsymbol{x}^{(j)} - \\sum_{i=1}^{m} \\alpha_i \\\\\n\\mathcal{s.t.} \\qquad \\sum_{i=1}^{m} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad 0 \\le \\alpha_i \\le C\n$$\n\n如果要引入到高维空间的映射，问题变为：\n\n$$\n\\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\color{red}{\\phi(\\boldsymbol{x}^{(i)})^{\\top} \\phi(\\boldsymbol{x}^{(j)})} - \\sum_{i=1}^{m} \\alpha_i \\\\\n =  \\min_{\\boldsymbol{\\alpha}: \\alpha_i \\ge 0} \\frac{1}{2}\\sum_{i=1}^{m} \\sum_{j=1}^{m} y^{(i)} y^{(j)} \\alpha_i \\alpha_j \\color{red}{K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(j)})} - \\sum_{i=1}^{m} \\alpha_i \\\\\n\\mathcal{s.t.} \\qquad \\sum_{i=1}^{m} \\alpha_i y^{(i)}=0 \\quad \\mathrm{and} \\quad 0 \\le \\alpha_i \\le C\n$$\n\n我们不需要去找到复杂的$\\phi$映射，因为在SVM的优化问题中，我们只需要计算内积，因此，我们只需要找到一个核函数\n$$\nK(\\boldsymbol{x}, \\boldsymbol{z}) = \\phi(\\boldsymbol{x})^{\\top}\\phi(\\boldsymbol{z})\n$$\n\n\n$\\phi$是复杂的，但是核函数是相对容易的，我们在课上给出了四种核函数：\n\n- RBF核: $\\exp\\left(-\\frac{\\|\\boldsymbol{x}-\\boldsymbol{z}\\|}{2\\sigma^2}\\right)$\n\n- 多项式核: $\\left(\\boldsymbol{x}^\\top\\boldsymbol{z}\\right)^d$\n\n- 余弦相似度核: $\\frac{\\boldsymbol{x}^\\top\\boldsymbol{z}}{\\|\\boldsymbol{x}\\|\\cdot\\|\\boldsymbol{z}\\|}$\n\n- sigmoid核: $\\tanh(\\alpha\\boldsymbol{x}^\\top\\boldsymbol{z} + c)$\n\n现在请实现这些其中常用的RBF,多项式，余弦相似度核函数"},{"cell_type":"code","execution_count":34,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"49EA9797A7B945A6838AC99FC4F0FA91","collapsed":false,"scrolled":false},"outputs":[],"source":"def poly_ker_wrap(d):\n    \"\"\"\n    polynomial kernel\n    - d: degree\n    Returns:\n    - ker: the kernel function\n    \"\"\"\n    #############################################################################\n    # DONE:                                                                     #\n    # Implement polynomial kernel                                               #\n    #############################################################################\n    def ker(x, z):\n        return (x.dot(z.T)) ** d\n    #############################################################################\n    #                             END OF YOUR CODE                              #\n    #############################################################################\n    return ker\n    \ndef cos_ker_wrap():\n    \"\"\"\n    cosine similarity kernel\n    Returns:\n    - ker: the kernel function\n    \"\"\"\n    #############################################################################\n    # DONE:                                                                     #\n    # Implement cosine similarity kernel                                        #\n    #############################################################################\n    def ker(x, z):\n        return (x.dot(z.T)) / (np.sqrt(x.dot(x.T)) * np.sqrt(z.dot(z.T)))\n    #############################################################################\n    #                             END OF YOUR CODE                              #\n    #############################################################################\n    return ker\n    \ndef rbf_ker_wrap(sigma):\n    \"\"\"\n    RBF kernel\n    - sigma: variance\n    Returns:\n    - ker: the kernel function\n    \"\"\"\n    #############################################################################\n    # DONE:                                                                     #\n    # Implement RBF kernel                                                      #\n    #############################################################################\n    def ker(x,z):\n        return np.exp( - (x-z).dot((x-z).T)  / (2.0 * sigma ** 2) )\n    #############################################################################\n    #                             END OF YOUR CODE                              #\n    #############################################################################\n    return ker"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"22B757C38C634139827FC3CAE67101F1","mdEditEnable":false},"source":"RBF核是一个非常常用的核函数，对应的映射函数可以将数据点映射到无线维,对于上面这个线性不可分的数据，RBF核可以起到非常好的效果，运行下面的代码进行比较，你可能需要调整训练轮数来得到比较好的效果。"},{"cell_type":"code","execution_count":35,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"6AB941BF776C4DC68E9AE4ADC4D85089","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 864x432 with 2 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/6AB941BF776C4DC68E9AE4ADC4D85089/q8l0gr4560.png\">"},"transient":{}}],"source":"from matplotlib import cm\ndef plot_boundary(ax, model, x, title):\n    y = model(x)\n    y[y < 0], y[y >= 0] = -1, 1\n\n    category = {'+1': [], '-1': []}\n    for point, label in zip(x, y):\n        if label == 1.0: category['+1'].append(point)\n        else: category['-1'].append(point)\n    for label, pts in category.items():\n        pts = np.array(pts)\n        ax.scatter(pts[:, 0], pts[:, 1], label=label)\n    \n    # plot boundary\n    p = np.meshgrid(np.arange(-1.5, 1.5, 0.025), np.arange(-1.5, 1.5, 0.025))\n    x = np.array([p[0].flatten(), p[1].flatten()]).T\n    y = model(x)\n    y[y < 0], y[y >= 0] = -1, 1\n    y = np.reshape(y, p[0].shape)\n    ax.contourf(p[0], p[1], y, cmap=plt.cm.coolwarm, alpha=0.4)\n    \n    # set title\n    ax.set_title(title)\n\nfig = plt.figure(figsize=(12,6))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\ndef get_model(x_data_used, y_data_used, alpha, b, kernel):\n    def model(X_in):\n        results = []\n        for k in range(X_in.shape[0]):\n            result = b\n            for i in range(x_data_used.shape[0]):\n                result += y_data_used[i] * alpha[i] * kernel(x_data_used[i], X_in[k])\n            results.append(result)\n        return np.array(results)\n    return model\n\n\n# modify the max iteration and numerical tolerance to achieve better performance\nalpha, b = svm_smo(x_sp, y_sp, 1e10, 1000, 1e-4, default_kernel)\nmodel = get_model(x_sp, y_sp, alpha, b, default_kernel)\nplot_boundary(ax1, model, x_sp, 'Default SVM')\n\nker = rbf_ker_wrap(0.2)\n\nalpha, b = svm_smo(x_sp, y_sp, 1e10, 1000, 1e-4, ker, isDefault = False)\nmodel = get_model(x_sp, y_sp, alpha, b, ker)\nplot_boundary(ax2, model, x_sp, 'SVM + RBF')\n\nplt.show()"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"A19B5F6C152A45729C471F2D44EAA4DB","mdEditEnable":false},"source":"至此，你已经看到了SVM + 核方法的威力，并且完成了本次昨夜的大部分内容，最后，我们来应用强大的机器学习库scikit-learn"},{"cell_type":"markdown","metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"5A0E82470EDA4F5F8CAD64BEF6916563"},"source":"## Scikit-Learn 中的 SVM\n\n在这一部分里，你们需要利用scikit-learn包，选择适当的参数，来训练一个使用RBF核函数的SVM，请参考 [scikit-learn API](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)"},{"cell_type":"code","execution_count":36,"metadata":{"jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"},"id":"367565D55EA943AF8E6DEBDA80A66021","collapsed":false,"scrolled":false},"outputs":[{"output_type":"display_data","metadata":{"needs_background":"light"},"data":{"text/plain":"<Figure size 432x432 with 1 Axes>","text/html":"<img src=\"https://cdn.kesci.com/rt_upload/367565D55EA943AF8E6DEBDA80A66021/q8h6czna1z.png\">"},"transient":{}}],"source":"from sklearn import svm\n\nmodel = None\ndata_x = x_sp\ndata_y = y_sp\n#############################################################################\n# DONE:                                                                     #\n# Implement scikit learn SVM with RBF kernel                                #\n#############################################################################\n\nmodel = svm.SVC(kernel = 'rbf', tol = 1e-6, gamma = 40)\nmodel.fit(data_x, data_y)\n\n#############################################################################\n#                             END OF YOUR CODE                              #\n#############################################################################\n\nfig = plt.figure(figsize=(6,6))\nax = fig.add_subplot(111)\nplot_boundary(ax, model.predict, data_x, 'SVM + RBF')\nplt.show()"},{"metadata":{"id":"3AE38F190AFD4F058D480DF6A38C3237","jupyter":{},"tags":[],"slideshow":{"slide_type":"slide"}},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.3","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}